{"cells":[{"cell_type":"markdown","metadata":{"id":"quwHu5RREX5z"},"source":["# General introduction\n","\n","In this lab, you will learn how to use the search functionality in the software stack of MASE.\n","\n","There are in total 4 tasks you would need to finish.\n","\n","# Writing a search using MaseGraph Transforms\n","\n","In this section, our objective is to gain a comprehensive understanding of the construction of the current search function in Mase. To achieve this, we will require these essential components:\n","\n","- MaseGraph: This component should be already created in the preceding lab.\n","- Search space: This component encompasses and defines the various available search options.\n","- Search strategy: An implementation of a search algorithm.\n","- Runner: This vital component manages and executes training, evaluation, or both procedures while generating a quality metric.\n","\n","By analyzing these components, we can delve into the workings and effectiveness of the existing search function in Mase."]},{"cell_type":"markdown","metadata":{"id":"G2zdGe7VA82g"},"source":["#Turning your network to a graph\n","\n","We follow a similar procedure of what you have tried in lab2 to now produce a MaseGraph, this is converted from your pre-trained JSC model:"]},{"cell_type":"code","execution_count":31,"metadata":{},"outputs":[],"source":["import sys\n","sys.path.append('/home/super_monkey/mase_new/machop')"]},{"cell_type":"code","execution_count":32,"metadata":{"id":"Z16mnfS8BM9o"},"outputs":[{"name":"stderr","output_type":"stream","text":["\u001b[32mINFO    \u001b[0m \u001b[34mSet logging level to info\u001b[0m\n","\u001b[32mINFO    \u001b[0m \u001b[34mLoaded pytorch lightning checkpoint from /home/super_monkey/mase_new/mase_output/jsc-tiny_classification_jsc_Base/software/training_ckpts/best.ckpt\u001b[0m\n"]}],"source":["import sys\n","import logging\n","import os\n","from pathlib import Path\n","from pprint import pprint as pp\n","\n","# # figure out the correct path\n","# machop_path = Path(\".\").resolve().parent.parent /\"machop\"\n","# assert machop_path.exists(), \"Failed to find machop at: {}\".format(machop_path)\n","# sys.path.append(str(machop_path))\n","\n","from chop.dataset import MaseDataModule, get_dataset_info\n","from chop.tools.logger import set_logging_verbosity\n","\n","from chop.passes.graph.analysis import (\n","    report_node_meta_param_analysis_pass,\n","    profile_statistics_analysis_pass,\n",")\n","from chop.passes.graph import (\n","    add_common_metadata_analysis_pass,\n","    init_metadata_analysis_pass,\n","    add_software_metadata_analysis_pass,\n",")\n","from chop.tools.get_input import InputGenerator\n","from chop.ir.graph.mase_graph import MaseGraph\n","\n","from chop.models import get_model_info, get_model\n","from chop.tools.checkpoint_load import load_model\n","\n","\n","\n","\n","set_logging_verbosity(\"info\")\n","\n","batch_size = 8\n","model_name = \"jsc-tiny\"\n","dataset_name = \"jsc\"\n","\n","\n","data_module = MaseDataModule(\n","    name=dataset_name,\n","    batch_size=batch_size,\n","    model_name=model_name,\n","    num_workers=0,\n","    # custom_dataset_cache_path=\"../../chop/dataset\"\n",")\n","data_module.prepare_data()\n","data_module.setup()\n","\n","CHECKPOINT_PATH = \"/home/super_monkey/mase_new/mase_output/jsc-tiny_classification_jsc_Base/software/training_ckpts/best.ckpt\"\n","model_info = get_model_info(model_name)\n","model = get_model(\n","    model_name,\n","    task=\"cls\",\n","    dataset_info=data_module.dataset_info,\n","    pretrained=False,\n","    checkpoint = None)\n","model = load_model(load_name=CHECKPOINT_PATH, load_type=\"pl\", model=model)\n","\n","input_generator = InputGenerator(\n","    data_module=data_module,\n","    model_info=model_info,\n","    task=\"cls\",\n","    which_dataloader=\"train\",\n",")\n","\n","dummy_in = next(iter(input_generator))\n","_ = model(**dummy_in)\n","\n","# generate the mase graph and initialize node metadata\n","mg = MaseGraph(model=model)"]},{"cell_type":"markdown","metadata":{"id":"Lxs3Co8qY7Ea"},"source":["#Defining a search space\n","\n","Based on the previous `pass_args` template, the following code is utilized to generate a search space. The search space is constructed by combining different weight and data configurations in precision setups."]},{"cell_type":"code","execution_count":34,"metadata":{"id":"JikYP98wZJPx"},"outputs":[],"source":["# from pprint import pformat\n","pass_args = {\n","\"by\": \"type\",\n","\"default\": {\"config\": {\"name\": None}},\n","\"linear\": {\n","        \"config\": {\n","            \"name\": \"integer\",\n","            # data\n","            \"data_in_width\": 8,\n","            \"data_in_frac_width\": 4,\n","            # weight\n","            \"weight_width\": 8,\n","            \"weight_frac_width\": 4,\n","            # bias\n","            \"bias_width\": 8,\n","            \"bias_frac_width\": 4,\n","        }\n","},}\n","\n","import copy\n","# build a search space\n","data_in_frac_widths = [(16, 8), (8, 6), (8, 4), (4, 2)]\n","w_in_frac_widths = [(16, 8), (8, 6), (8, 4), (4, 2)]\n","search_spaces = []\n","search_history=[]\n","for d_config in data_in_frac_widths:\n","    for w_config in w_in_frac_widths:\n","        pass_args['linear']['config']['data_in_width'] = d_config[0]\n","        pass_args['linear']['config']['data_in_frac_width'] = d_config[1]\n","        pass_args['linear']['config']['weight_width'] = w_config[0]\n","        pass_args['linear']['config']['weight_frac_width'] = w_config[1]\n","        # dict.copy() and dict(dict) only perform shallow copies\n","        # in fact, only primitive data types in python are doing implicit copy when a = b happens\n","        search_spaces.append(copy.deepcopy(pass_args))\n","        search_history.append([d_config,w_config])\n","        # print(pformat(search_spaces))"]},{"cell_type":"markdown","metadata":{"id":"H8XLOES6ZWOz"},"source":["## Defining a search strategy and a runner\n","\n","The code provided below consists of two main `for` loops. The first `for` loop executes a straightforward brute-force search, enabling the iteration through the previously defined search space.\n","\n","In contrast, the second `for` loop retrieves training samples from the train data loader. These samples are then utilized to generate accuracy and loss values, which serve as potential quality metrics for evaluating the system's performance.\n"]},{"cell_type":"code","execution_count":35,"metadata":{"id":"pjbFnH-hZZ55"},"outputs":[],"source":["# grid search\n","import torch\n","import numpy as np\n","from torchmetrics.classification import MulticlassAccuracy\n","\n","from chop.passes.graph.transforms import (\n","    quantize_transform_pass,\n","    summarize_quantization_analysis_pass,\n",")\n","\n","mg, _ = init_metadata_analysis_pass(mg, None)\n","mg, _ = add_common_metadata_analysis_pass(mg, {\"dummy_in\": dummy_in})\n","mg, _ = add_software_metadata_analysis_pass(mg, None)"]},{"cell_type":"markdown","metadata":{"id":"67C6-KNkamKF"},"source":["We now have the following task for you:\n","\n","1. Explore additional metrics that can serve as quality metrics for the search process. For example, you can consider metrics such as latency, model size, or the number of FLOPs (floating-point operations) involved in the model.\n"]},{"cell_type":"code","execution_count":36,"metadata":{},"outputs":[],"source":["from deepspeed.profiling.flops_profiler import get_model_profile\n"]},{"cell_type":"code","execution_count":39,"metadata":{},"outputs":[],"source":["# from pprint import pformat\n","pass_args = {\n","\"by\": \"type\",\n","\"default\": {\"config\": {\"name\": None}},\n","\"linear\": {\n","        \"config\": {\n","            \"name\": \"integer\",\n","            # data\n","            \"data_in_width\": 8,\n","            \"data_in_frac_width\": 4,\n","            # weight\n","            \"weight_width\": 8,\n","            \"weight_frac_width\": 4,\n","            # bias\n","            \"bias_width\": 8,\n","            \"bias_frac_width\": 4,\n","        }\n","},}\n","\n","import copy\n","# build a search space\n","data_in_frac_widths = [(16, 8), (8, 6), (8, 4), (4, 2)]\n","w_in_frac_widths = [(16, 8), (8, 6), (8, 4), (4, 2)]\n","search_spaces = []\n","for d_config in data_in_frac_widths:\n","    for w_config in w_in_frac_widths:\n","        pass_args['linear']['config']['data_in_width'] = d_config[0]\n","        pass_args['linear']['config']['data_in_frac_width'] = d_config[1]\n","        pass_args['linear']['config']['weight_width'] = w_config[0]\n","        pass_args['linear']['config']['weight_frac_width'] = w_config[1]\n","        # dict.copy() and dict(dict) only perform shallow copies\n","        # in fact, only primitive data types in python are doing implicit copy when a = b happens\n","        search_spaces.append(copy.deepcopy(pass_args))\n","        # print(pformat(search_spaces))"]},{"cell_type":"code","execution_count":40,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["torch.Size([8, 16])\n","[2024-02-11 16:16:23,199] [INFO] [profiler.py:1205:get_model_profile] Flops profiler warming-up...\n","[2024-02-11 16:16:23,201] [INFO] [profiler.py:80:start_profile] Flops profiler started\n","\n","-------------------------- DeepSpeed Flops Profiler --------------------------\n","Profile Summary at step 1:\n","Notations:\n","data parallel size (dp_size), model parallel size(mp_size),\n","number of parameters (params), number of multiply-accumulate operations(MACs),\n","number of floating-point operations (flops), floating-point operations per second (FLOPS),\n","fwd latency (forward propagation latency), bwd latency (backward propagation latency),\n","step (weights update latency), iter latency (sum of fwd, bwd and step latency)\n","\n","params per GPU:                                                         117     \n","params of model = params per GPU * mp_size:                             0       \n","fwd MACs per GPU:                                                       640 MACs\n","fwd flops per GPU:                                                      1.92 K  \n","fwd flops of model = fwd flops per GPU * mp_size:                       1.92 K  \n","fwd latency:                                                            503.3 us\n","fwd FLOPS per GPU = fwd flops per GPU / fwd latency:                    3.81 MFLOPS\n","\n","----------------------------- Aggregated Profile per GPU -----------------------------\n","Top 1 modules in terms of params, MACs or fwd latency at different model depths:\n","depth 0:\n","    params      - {'GraphModule': '117'}\n","    MACs        - {'GraphModule': '640 MACs'}\n","    fwd latency - {'GraphModule': '503.3 us'}\n","depth 1:\n","    params      - {'Module': '117'}\n","    MACs        - {'Module': '640 MACs'}\n","    fwd latency - {'Module': '401.5 us'}\n","\n","------------------------------ Detailed Profile per GPU ------------------------------\n","Each module profile is listed after its name in the following order: \n","params, percentage of total params, MACs, percentage of total MACs, fwd latency, percentage of total fwd latency, fwd FLOPS\n","\n","Note: 1. A module can have torch.nn.module or torch.nn.functional to compute logits (e.g. CrossEntropyLoss). They are not counted as submodules, thus not to be printed out. However they make up the difference between a parent's MACs (or latency) and the sum of its submodules'.\n","2. Number of floating-point operations is a theoretical estimation, thus FLOPS computed using that could be larger than the maximum system throughput.\n","3. The fwd latency listed in the top module's profile is directly captured at the module forward function in PyTorch, thus it's less than the fwd latency shown above which is captured in DeepSpeed.\n","\n","GraphModule(\n","  117 = 100% Params, 640 MACs = 100% MACs, 503.3 us = 100% latency, 3.81 MFLOPS\n","  (seq_blocks): Module(\n","    117 = 100% Params, 640 MACs = 100% MACs, 401.5 us = 79.77% latency, 4.77 MFLOPS\n","    (0): BatchNorm1d(32 = 27.35% Params, 0 MACs = 0% MACs, 66.76 us = 13.26% latency, 3.83 MFLOPS, 16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","    (1): ReLU(0 = 0% Params, 0 MACs = 0% MACs, 37.19 us = 7.39% latency, 3.44 MFLOPS, inplace=True)\n","    (2): LinearInteger(85 = 72.65% Params, 640 MACs = 100% MACs, 263.93 us = 52.44% latency, 5.66 MFLOPS, in_features=16, out_features=5, bias=True)\n","    (3): ReLU(0 = 0% Params, 0 MACs = 0% MACs, 33.62 us = 6.68% latency, 1.19 MFLOPS, inplace=True)\n","  )\n",")\n","\n","\n","\n","def forward(self, x):\n","    seq_blocks_0 = getattr(self.seq_blocks, \"0\")(x);  x = None\n","    seq_blocks_1 = getattr(self.seq_blocks, \"1\")(seq_blocks_0);  seq_blocks_0 = None\n","    seq_blocks_2 = getattr(self.seq_blocks, \"2\")(seq_blocks_1);  seq_blocks_1 = None\n","    seq_blocks_3 = getattr(self.seq_blocks, \"3\")(seq_blocks_2);  seq_blocks_2 = None\n","    return seq_blocks_3\n","    \n","# To see more debug info, please use `graph_module.print_readable()`\n","------------------------------------------------------------------------------\n","[2024-02-11 16:16:23,203] [INFO] [profiler.py:226:end_profile] Flops profiler finished\n","torch.Size([8, 16])\n","[2024-02-11 16:16:23,251] [INFO] [profiler.py:1205:get_model_profile] Flops profiler warming-up...\n","[2024-02-11 16:16:23,252] [INFO] [profiler.py:80:start_profile] Flops profiler started\n","\n","-------------------------- DeepSpeed Flops Profiler --------------------------\n","Profile Summary at step 1:\n","Notations:\n","data parallel size (dp_size), model parallel size(mp_size),\n","number of parameters (params), number of multiply-accumulate operations(MACs),\n","number of floating-point operations (flops), floating-point operations per second (FLOPS),\n","fwd latency (forward propagation latency), bwd latency (backward propagation latency),\n","step (weights update latency), iter latency (sum of fwd, bwd and step latency)\n","\n","params per GPU:                                                         117     \n","params of model = params per GPU * mp_size:                             0       \n","fwd MACs per GPU:                                                       640 MACs\n","fwd flops per GPU:                                                      1.92 K  \n","fwd flops of model = fwd flops per GPU * mp_size:                       1.92 K  \n","fwd latency:                                                            612.5 us\n","fwd FLOPS per GPU = fwd flops per GPU / fwd latency:                    3.13 MFLOPS\n","\n","----------------------------- Aggregated Profile per GPU -----------------------------\n","Top 1 modules in terms of params, MACs or fwd latency at different model depths:\n","depth 0:\n","    params      - {'GraphModule': '117'}\n","    MACs        - {'GraphModule': '640 MACs'}\n","    fwd latency - {'GraphModule': '612.5 us'}\n","depth 1:\n","    params      - {'Module': '117'}\n","    MACs        - {'Module': '640 MACs'}\n","    fwd latency - {'Module': '510.69 us'}\n","\n","------------------------------ Detailed Profile per GPU ------------------------------\n","Each module profile is listed after its name in the following order: \n","params, percentage of total params, MACs, percentage of total MACs, fwd latency, percentage of total fwd latency, fwd FLOPS\n","\n","Note: 1. A module can have torch.nn.module or torch.nn.functional to compute logits (e.g. CrossEntropyLoss). They are not counted as submodules, thus not to be printed out. However they make up the difference between a parent's MACs (or latency) and the sum of its submodules'.\n","2. Number of floating-point operations is a theoretical estimation, thus FLOPS computed using that could be larger than the maximum system throughput.\n","3. The fwd latency listed in the top module's profile is directly captured at the module forward function in PyTorch, thus it's less than the fwd latency shown above which is captured in DeepSpeed.\n","\n","GraphModule(\n","  117 = 100% Params, 640 MACs = 100% MACs, 612.5 us = 100% latency, 3.13 MFLOPS\n","  (seq_blocks): Module(\n","    117 = 100% Params, 640 MACs = 100% MACs, 510.69 us = 83.38% latency, 3.75 MFLOPS\n","    (0): BatchNorm1d(32 = 27.35% Params, 0 MACs = 0% MACs, 148.77 us = 24.29% latency, 1.72 MFLOPS, 16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","    (1): ReLU(0 = 0% Params, 0 MACs = 0% MACs, 35.29 us = 5.76% latency, 3.63 MFLOPS, inplace=True)\n","    (2): LinearInteger(85 = 72.65% Params, 640 MACs = 100% MACs, 292.3 us = 47.72% latency, 5.11 MFLOPS, in_features=16, out_features=5, bias=True)\n","    (3): ReLU(0 = 0% Params, 0 MACs = 0% MACs, 34.33 us = 5.61% latency, 1.17 MFLOPS, inplace=True)\n","  )\n",")\n","\n","\n","\n","def forward(self, x):\n","    seq_blocks_0 = getattr(self.seq_blocks, \"0\")(x);  x = None\n","    seq_blocks_1 = getattr(self.seq_blocks, \"1\")(seq_blocks_0);  seq_blocks_0 = None\n","    seq_blocks_2 = getattr(self.seq_blocks, \"2\")(seq_blocks_1);  seq_blocks_1 = None\n","    seq_blocks_3 = getattr(self.seq_blocks, \"3\")(seq_blocks_2);  seq_blocks_2 = None\n","    return seq_blocks_3\n","    \n","# To see more debug info, please use `graph_module.print_readable()`\n","------------------------------------------------------------------------------\n","[2024-02-11 16:16:23,254] [INFO] [profiler.py:226:end_profile] Flops profiler finished\n","torch.Size([8, 16])\n","[2024-02-11 16:16:23,299] [INFO] [profiler.py:1205:get_model_profile] Flops profiler warming-up...\n","[2024-02-11 16:16:23,300] [INFO] [profiler.py:80:start_profile] Flops profiler started\n","\n","-------------------------- DeepSpeed Flops Profiler --------------------------\n","Profile Summary at step 1:\n","Notations:\n","data parallel size (dp_size), model parallel size(mp_size),\n","number of parameters (params), number of multiply-accumulate operations(MACs),\n","number of floating-point operations (flops), floating-point operations per second (FLOPS),\n","fwd latency (forward propagation latency), bwd latency (backward propagation latency),\n","step (weights update latency), iter latency (sum of fwd, bwd and step latency)\n","\n","params per GPU:                                                         117     \n","params of model = params per GPU * mp_size:                             0       \n","fwd MACs per GPU:                                                       640 MACs\n","fwd flops per GPU:                                                      1.92 K  \n","fwd flops of model = fwd flops per GPU * mp_size:                       1.92 K  \n","fwd latency:                                                            679.49 us\n","fwd FLOPS per GPU = fwd flops per GPU / fwd latency:                    2.82 MFLOPS\n","\n","----------------------------- Aggregated Profile per GPU -----------------------------\n","Top 1 modules in terms of params, MACs or fwd latency at different model depths:\n","depth 0:\n","    params      - {'GraphModule': '117'}\n","    MACs        - {'GraphModule': '640 MACs'}\n","    fwd latency - {'GraphModule': '679.49 us'}\n","depth 1:\n","    params      - {'Module': '117'}\n","    MACs        - {'Module': '640 MACs'}\n","    fwd latency - {'Module': '546.93 us'}\n","\n","------------------------------ Detailed Profile per GPU ------------------------------\n","Each module profile is listed after its name in the following order: \n","params, percentage of total params, MACs, percentage of total MACs, fwd latency, percentage of total fwd latency, fwd FLOPS\n","\n","Note: 1. A module can have torch.nn.module or torch.nn.functional to compute logits (e.g. CrossEntropyLoss). They are not counted as submodules, thus not to be printed out. However they make up the difference between a parent's MACs (or latency) and the sum of its submodules'.\n","2. Number of floating-point operations is a theoretical estimation, thus FLOPS computed using that could be larger than the maximum system throughput.\n","3. The fwd latency listed in the top module's profile is directly captured at the module forward function in PyTorch, thus it's less than the fwd latency shown above which is captured in DeepSpeed.\n","\n","GraphModule(\n","  117 = 100% Params, 640 MACs = 100% MACs, 679.49 us = 100% latency, 2.82 MFLOPS\n","  (seq_blocks): Module(\n","    117 = 100% Params, 640 MACs = 100% MACs, 546.93 us = 80.49% latency, 3.51 MFLOPS\n","    (0): BatchNorm1d(32 = 27.35% Params, 0 MACs = 0% MACs, 162.6 us = 23.93% latency, 1.57 MFLOPS, 16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","    (1): ReLU(0 = 0% Params, 0 MACs = 0% MACs, 37.91 us = 5.58% latency, 3.38 MFLOPS, inplace=True)\n","    (2): LinearInteger(85 = 72.65% Params, 640 MACs = 100% MACs, 285.63 us = 42.04% latency, 5.23 MFLOPS, in_features=16, out_features=5, bias=True)\n","    (3): ReLU(0 = 0% Params, 0 MACs = 0% MACs, 60.8 us = 8.95% latency, 657.93 KFLOPS, inplace=True)\n","  )\n",")\n","\n","\n","\n","def forward(self, x):\n","    seq_blocks_0 = getattr(self.seq_blocks, \"0\")(x);  x = None\n","    seq_blocks_1 = getattr(self.seq_blocks, \"1\")(seq_blocks_0);  seq_blocks_0 = None\n","    seq_blocks_2 = getattr(self.seq_blocks, \"2\")(seq_blocks_1);  seq_blocks_1 = None\n","    seq_blocks_3 = getattr(self.seq_blocks, \"3\")(seq_blocks_2);  seq_blocks_2 = None\n","    return seq_blocks_3\n","    \n","# To see more debug info, please use `graph_module.print_readable()`\n","------------------------------------------------------------------------------\n","[2024-02-11 16:16:23,302] [INFO] [profiler.py:226:end_profile] Flops profiler finished\n","torch.Size([8, 16])\n","[2024-02-11 16:16:23,347] [INFO] [profiler.py:1205:get_model_profile] Flops profiler warming-up...\n","[2024-02-11 16:16:23,347] [INFO] [profiler.py:80:start_profile] Flops profiler started\n","\n","-------------------------- DeepSpeed Flops Profiler --------------------------\n","Profile Summary at step 1:\n","Notations:\n","data parallel size (dp_size), model parallel size(mp_size),\n","number of parameters (params), number of multiply-accumulate operations(MACs),\n","number of floating-point operations (flops), floating-point operations per second (FLOPS),\n","fwd latency (forward propagation latency), bwd latency (backward propagation latency),\n","step (weights update latency), iter latency (sum of fwd, bwd and step latency)\n","\n","params per GPU:                                                         117     \n","params of model = params per GPU * mp_size:                             0       \n","fwd MACs per GPU:                                                       640 MACs\n","fwd flops per GPU:                                                      1.92 K  \n","fwd flops of model = fwd flops per GPU * mp_size:                       1.92 K  \n","fwd latency:                                                            515.46 us\n","fwd FLOPS per GPU = fwd flops per GPU / fwd latency:                    3.72 MFLOPS\n","\n","----------------------------- Aggregated Profile per GPU -----------------------------\n","Top 1 modules in terms of params, MACs or fwd latency at different model depths:\n","depth 0:\n","    params      - {'GraphModule': '117'}\n","    MACs        - {'GraphModule': '640 MACs'}\n","    fwd latency - {'GraphModule': '515.46 us'}\n","depth 1:\n","    params      - {'Module': '117'}\n","    MACs        - {'Module': '640 MACs'}\n","    fwd latency - {'Module': '402.93 us'}\n","\n","------------------------------ Detailed Profile per GPU ------------------------------\n","Each module profile is listed after its name in the following order: \n","params, percentage of total params, MACs, percentage of total MACs, fwd latency, percentage of total fwd latency, fwd FLOPS\n","\n","Note: 1. A module can have torch.nn.module or torch.nn.functional to compute logits (e.g. CrossEntropyLoss). They are not counted as submodules, thus not to be printed out. However they make up the difference between a parent's MACs (or latency) and the sum of its submodules'.\n","2. Number of floating-point operations is a theoretical estimation, thus FLOPS computed using that could be larger than the maximum system throughput.\n","3. The fwd latency listed in the top module's profile is directly captured at the module forward function in PyTorch, thus it's less than the fwd latency shown above which is captured in DeepSpeed.\n","\n","GraphModule(\n","  117 = 100% Params, 640 MACs = 100% MACs, 515.46 us = 100% latency, 3.72 MFLOPS\n","  (seq_blocks): Module(\n","    117 = 100% Params, 640 MACs = 100% MACs, 402.93 us = 78.17% latency, 4.76 MFLOPS\n","    (0): BatchNorm1d(32 = 27.35% Params, 0 MACs = 0% MACs, 66.52 us = 12.9% latency, 3.85 MFLOPS, 16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","    (1): ReLU(0 = 0% Params, 0 MACs = 0% MACs, 39.34 us = 7.63% latency, 3.25 MFLOPS, inplace=True)\n","    (2): LinearInteger(85 = 72.65% Params, 640 MACs = 100% MACs, 259.64 us = 50.37% latency, 5.75 MFLOPS, in_features=16, out_features=5, bias=True)\n","    (3): ReLU(0 = 0% Params, 0 MACs = 0% MACs, 37.43 us = 7.26% latency, 1.07 MFLOPS, inplace=True)\n","  )\n",")\n","\n","\n","\n","def forward(self, x):\n","    seq_blocks_0 = getattr(self.seq_blocks, \"0\")(x);  x = None\n","    seq_blocks_1 = getattr(self.seq_blocks, \"1\")(seq_blocks_0);  seq_blocks_0 = None\n","    seq_blocks_2 = getattr(self.seq_blocks, \"2\")(seq_blocks_1);  seq_blocks_1 = None\n","    seq_blocks_3 = getattr(self.seq_blocks, \"3\")(seq_blocks_2);  seq_blocks_2 = None\n","    return seq_blocks_3\n","    \n","# To see more debug info, please use `graph_module.print_readable()`\n","------------------------------------------------------------------------------\n","[2024-02-11 16:16:23,350] [INFO] [profiler.py:226:end_profile] Flops profiler finished\n","torch.Size([8, 16])\n","[2024-02-11 16:16:23,391] [INFO] [profiler.py:1205:get_model_profile] Flops profiler warming-up...\n","[2024-02-11 16:16:23,392] [INFO] [profiler.py:80:start_profile] Flops profiler started\n","\n","-------------------------- DeepSpeed Flops Profiler --------------------------\n","Profile Summary at step 1:\n","Notations:\n","data parallel size (dp_size), model parallel size(mp_size),\n","number of parameters (params), number of multiply-accumulate operations(MACs),\n","number of floating-point operations (flops), floating-point operations per second (FLOPS),\n","fwd latency (forward propagation latency), bwd latency (backward propagation latency),\n","step (weights update latency), iter latency (sum of fwd, bwd and step latency)\n","\n","params per GPU:                                                         117     \n","params of model = params per GPU * mp_size:                             0       \n","fwd MACs per GPU:                                                       640 MACs\n","fwd flops per GPU:                                                      1.92 K  \n","fwd flops of model = fwd flops per GPU * mp_size:                       1.92 K  \n","fwd latency:                                                            630.38 us\n","fwd FLOPS per GPU = fwd flops per GPU / fwd latency:                    3.04 MFLOPS\n","\n","----------------------------- Aggregated Profile per GPU -----------------------------\n","Top 1 modules in terms of params, MACs or fwd latency at different model depths:\n","depth 0:\n","    params      - {'GraphModule': '117'}\n","    MACs        - {'GraphModule': '640 MACs'}\n","    fwd latency - {'GraphModule': '630.38 us'}\n","depth 1:\n","    params      - {'Module': '117'}\n","    MACs        - {'Module': '640 MACs'}\n","    fwd latency - {'Module': '498.06 us'}\n","\n","------------------------------ Detailed Profile per GPU ------------------------------\n","Each module profile is listed after its name in the following order: \n","params, percentage of total params, MACs, percentage of total MACs, fwd latency, percentage of total fwd latency, fwd FLOPS\n","\n","Note: 1. A module can have torch.nn.module or torch.nn.functional to compute logits (e.g. CrossEntropyLoss). They are not counted as submodules, thus not to be printed out. However they make up the difference between a parent's MACs (or latency) and the sum of its submodules'.\n","2. Number of floating-point operations is a theoretical estimation, thus FLOPS computed using that could be larger than the maximum system throughput.\n","3. The fwd latency listed in the top module's profile is directly captured at the module forward function in PyTorch, thus it's less than the fwd latency shown above which is captured in DeepSpeed.\n","\n","GraphModule(\n","  117 = 100% Params, 640 MACs = 100% MACs, 630.38 us = 100% latency, 3.04 MFLOPS\n","  (seq_blocks): Module(\n","    117 = 100% Params, 640 MACs = 100% MACs, 498.06 us = 79.01% latency, 3.85 MFLOPS\n","    (0): BatchNorm1d(32 = 27.35% Params, 0 MACs = 0% MACs, 64.85 us = 10.29% latency, 3.95 MFLOPS, 16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","    (1): ReLU(0 = 0% Params, 0 MACs = 0% MACs, 38.15 us = 6.05% latency, 3.36 MFLOPS, inplace=True)\n","    (2): LinearInteger(85 = 72.65% Params, 640 MACs = 100% MACs, 357.39 us = 56.69% latency, 4.18 MFLOPS, in_features=16, out_features=5, bias=True)\n","    (3): ReLU(0 = 0% Params, 0 MACs = 0% MACs, 37.67 us = 5.98% latency, 1.06 MFLOPS, inplace=True)\n","  )\n",")\n","\n","\n","\n","def forward(self, x):\n","    seq_blocks_0 = getattr(self.seq_blocks, \"0\")(x);  x = None\n","    seq_blocks_1 = getattr(self.seq_blocks, \"1\")(seq_blocks_0);  seq_blocks_0 = None\n","    seq_blocks_2 = getattr(self.seq_blocks, \"2\")(seq_blocks_1);  seq_blocks_1 = None\n","    seq_blocks_3 = getattr(self.seq_blocks, \"3\")(seq_blocks_2);  seq_blocks_2 = None\n","    return seq_blocks_3\n","    \n","# To see more debug info, please use `graph_module.print_readable()`\n","------------------------------------------------------------------------------\n","[2024-02-11 16:16:23,395] [INFO] [profiler.py:226:end_profile] Flops profiler finished\n","torch.Size([8, 16])\n","[2024-02-11 16:16:23,440] [INFO] [profiler.py:1205:get_model_profile] Flops profiler warming-up...\n","[2024-02-11 16:16:23,440] [INFO] [profiler.py:80:start_profile] Flops profiler started\n","\n","-------------------------- DeepSpeed Flops Profiler --------------------------\n","Profile Summary at step 1:\n","Notations:\n","data parallel size (dp_size), model parallel size(mp_size),\n","number of parameters (params), number of multiply-accumulate operations(MACs),\n","number of floating-point operations (flops), floating-point operations per second (FLOPS),\n","fwd latency (forward propagation latency), bwd latency (backward propagation latency),\n","step (weights update latency), iter latency (sum of fwd, bwd and step latency)\n","\n","params per GPU:                                                         117     \n","params of model = params per GPU * mp_size:                             0       \n","fwd MACs per GPU:                                                       640 MACs\n","fwd flops per GPU:                                                      1.92 K  \n","fwd flops of model = fwd flops per GPU * mp_size:                       1.92 K  \n","fwd latency:                                                            578.88 us\n","fwd FLOPS per GPU = fwd flops per GPU / fwd latency:                    3.31 MFLOPS\n","\n","----------------------------- Aggregated Profile per GPU -----------------------------\n","Top 1 modules in terms of params, MACs or fwd latency at different model depths:\n","depth 0:\n","    params      - {'GraphModule': '117'}\n","    MACs        - {'GraphModule': '640 MACs'}\n","    fwd latency - {'GraphModule': '578.88 us'}\n","depth 1:\n","    params      - {'Module': '117'}\n","    MACs        - {'Module': '640 MACs'}\n","    fwd latency - {'Module': '475.41 us'}\n","\n","------------------------------ Detailed Profile per GPU ------------------------------\n","Each module profile is listed after its name in the following order: \n","params, percentage of total params, MACs, percentage of total MACs, fwd latency, percentage of total fwd latency, fwd FLOPS\n","\n","Note: 1. A module can have torch.nn.module or torch.nn.functional to compute logits (e.g. CrossEntropyLoss). They are not counted as submodules, thus not to be printed out. However they make up the difference between a parent's MACs (or latency) and the sum of its submodules'.\n","2. Number of floating-point operations is a theoretical estimation, thus FLOPS computed using that could be larger than the maximum system throughput.\n","3. The fwd latency listed in the top module's profile is directly captured at the module forward function in PyTorch, thus it's less than the fwd latency shown above which is captured in DeepSpeed.\n","\n","GraphModule(\n","  117 = 100% Params, 640 MACs = 100% MACs, 578.88 us = 100% latency, 3.31 MFLOPS\n","  (seq_blocks): Module(\n","    117 = 100% Params, 640 MACs = 100% MACs, 475.41 us = 82.13% latency, 4.03 MFLOPS\n","    (0): BatchNorm1d(32 = 27.35% Params, 0 MACs = 0% MACs, 156.4 us = 27.02% latency, 1.64 MFLOPS, 16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","    (1): ReLU(0 = 0% Params, 0 MACs = 0% MACs, 37.67 us = 6.51% latency, 3.4 MFLOPS, inplace=True)\n","    (2): LinearInteger(85 = 72.65% Params, 640 MACs = 100% MACs, 246.29 us = 42.55% latency, 6.06 MFLOPS, in_features=16, out_features=5, bias=True)\n","    (3): ReLU(0 = 0% Params, 0 MACs = 0% MACs, 35.05 us = 6.05% latency, 1.14 MFLOPS, inplace=True)\n","  )\n",")\n","\n","\n","\n","def forward(self, x):\n","    seq_blocks_0 = getattr(self.seq_blocks, \"0\")(x);  x = None\n","    seq_blocks_1 = getattr(self.seq_blocks, \"1\")(seq_blocks_0);  seq_blocks_0 = None\n","    seq_blocks_2 = getattr(self.seq_blocks, \"2\")(seq_blocks_1);  seq_blocks_1 = None\n","    seq_blocks_3 = getattr(self.seq_blocks, \"3\")(seq_blocks_2);  seq_blocks_2 = None\n","    return seq_blocks_3\n","    \n","# To see more debug info, please use `graph_module.print_readable()`\n","------------------------------------------------------------------------------\n","[2024-02-11 16:16:23,442] [INFO] [profiler.py:226:end_profile] Flops profiler finished\n","torch.Size([8, 16])\n","[2024-02-11 16:16:23,479] [INFO] [profiler.py:1205:get_model_profile] Flops profiler warming-up...\n","[2024-02-11 16:16:23,480] [INFO] [profiler.py:80:start_profile] Flops profiler started\n","\n","-------------------------- DeepSpeed Flops Profiler --------------------------\n","Profile Summary at step 1:\n","Notations:\n","data parallel size (dp_size), model parallel size(mp_size),\n","number of parameters (params), number of multiply-accumulate operations(MACs),\n","number of floating-point operations (flops), floating-point operations per second (FLOPS),\n","fwd latency (forward propagation latency), bwd latency (backward propagation latency),\n","step (weights update latency), iter latency (sum of fwd, bwd and step latency)\n","\n","params per GPU:                                                         117     \n","params of model = params per GPU * mp_size:                             0       \n","fwd MACs per GPU:                                                       640 MACs\n","fwd flops per GPU:                                                      1.92 K  \n","fwd flops of model = fwd flops per GPU * mp_size:                       1.92 K  \n","fwd latency:                                                            446.56 us\n","fwd FLOPS per GPU = fwd flops per GPU / fwd latency:                    4.29 MFLOPS\n","\n","----------------------------- Aggregated Profile per GPU -----------------------------\n","Top 1 modules in terms of params, MACs or fwd latency at different model depths:\n","depth 0:\n","    params      - {'GraphModule': '117'}\n","    MACs        - {'GraphModule': '640 MACs'}\n","    fwd latency - {'GraphModule': '446.56 us'}\n","depth 1:\n","    params      - {'Module': '117'}\n","    MACs        - {'Module': '640 MACs'}\n","    fwd latency - {'Module': '350.95 us'}\n","\n","------------------------------ Detailed Profile per GPU ------------------------------\n","Each module profile is listed after its name in the following order: \n","params, percentage of total params, MACs, percentage of total MACs, fwd latency, percentage of total fwd latency, fwd FLOPS\n","\n","Note: 1. A module can have torch.nn.module or torch.nn.functional to compute logits (e.g. CrossEntropyLoss). They are not counted as submodules, thus not to be printed out. However they make up the difference between a parent's MACs (or latency) and the sum of its submodules'.\n","2. Number of floating-point operations is a theoretical estimation, thus FLOPS computed using that could be larger than the maximum system throughput.\n","3. The fwd latency listed in the top module's profile is directly captured at the module forward function in PyTorch, thus it's less than the fwd latency shown above which is captured in DeepSpeed.\n","\n","GraphModule(\n","  117 = 100% Params, 640 MACs = 100% MACs, 446.56 us = 100% latency, 4.29 MFLOPS\n","  (seq_blocks): Module(\n","    117 = 100% Params, 640 MACs = 100% MACs, 350.95 us = 78.59% latency, 5.46 MFLOPS\n","    (0): BatchNorm1d(32 = 27.35% Params, 0 MACs = 0% MACs, 59.6 us = 13.35% latency, 4.29 MFLOPS, 16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","    (1): ReLU(0 = 0% Params, 0 MACs = 0% MACs, 33.38 us = 7.47% latency, 3.83 MFLOPS, inplace=True)\n","    (2): LinearInteger(85 = 72.65% Params, 640 MACs = 100% MACs, 226.26 us = 50.67% latency, 6.6 MFLOPS, in_features=16, out_features=5, bias=True)\n","    (3): ReLU(0 = 0% Params, 0 MACs = 0% MACs, 31.71 us = 7.1% latency, 1.26 MFLOPS, inplace=True)\n","  )\n",")\n","\n","\n","\n","def forward(self, x):\n","    seq_blocks_0 = getattr(self.seq_blocks, \"0\")(x);  x = None\n","    seq_blocks_1 = getattr(self.seq_blocks, \"1\")(seq_blocks_0);  seq_blocks_0 = None\n","    seq_blocks_2 = getattr(self.seq_blocks, \"2\")(seq_blocks_1);  seq_blocks_1 = None\n","    seq_blocks_3 = getattr(self.seq_blocks, \"3\")(seq_blocks_2);  seq_blocks_2 = None\n","    return seq_blocks_3\n","    \n","# To see more debug info, please use `graph_module.print_readable()`\n","------------------------------------------------------------------------------\n","[2024-02-11 16:16:23,482] [INFO] [profiler.py:226:end_profile] Flops profiler finished\n","torch.Size([8, 16])\n","[2024-02-11 16:16:23,525] [INFO] [profiler.py:1205:get_model_profile] Flops profiler warming-up...\n","[2024-02-11 16:16:23,526] [INFO] [profiler.py:80:start_profile] Flops profiler started\n","\n","-------------------------- DeepSpeed Flops Profiler --------------------------\n","Profile Summary at step 1:\n","Notations:\n","data parallel size (dp_size), model parallel size(mp_size),\n","number of parameters (params), number of multiply-accumulate operations(MACs),\n","number of floating-point operations (flops), floating-point operations per second (FLOPS),\n","fwd latency (forward propagation latency), bwd latency (backward propagation latency),\n","step (weights update latency), iter latency (sum of fwd, bwd and step latency)\n","\n","params per GPU:                                                         117     \n","params of model = params per GPU * mp_size:                             0       \n","fwd MACs per GPU:                                                       640 MACs\n","fwd flops per GPU:                                                      1.92 K  \n","fwd flops of model = fwd flops per GPU * mp_size:                       1.92 K  \n","fwd latency:                                                            689.51 us\n","fwd FLOPS per GPU = fwd flops per GPU / fwd latency:                    2.78 MFLOPS\n","\n","----------------------------- Aggregated Profile per GPU -----------------------------\n","Top 1 modules in terms of params, MACs or fwd latency at different model depths:\n","depth 0:\n","    params      - {'GraphModule': '117'}\n","    MACs        - {'GraphModule': '640 MACs'}\n","    fwd latency - {'GraphModule': '689.51 us'}\n","depth 1:\n","    params      - {'Module': '117'}\n","    MACs        - {'Module': '640 MACs'}\n","    fwd latency - {'Module': '573.16 us'}\n","\n","------------------------------ Detailed Profile per GPU ------------------------------\n","Each module profile is listed after its name in the following order: \n","params, percentage of total params, MACs, percentage of total MACs, fwd latency, percentage of total fwd latency, fwd FLOPS\n","\n","Note: 1. A module can have torch.nn.module or torch.nn.functional to compute logits (e.g. CrossEntropyLoss). They are not counted as submodules, thus not to be printed out. However they make up the difference between a parent's MACs (or latency) and the sum of its submodules'.\n","2. Number of floating-point operations is a theoretical estimation, thus FLOPS computed using that could be larger than the maximum system throughput.\n","3. The fwd latency listed in the top module's profile is directly captured at the module forward function in PyTorch, thus it's less than the fwd latency shown above which is captured in DeepSpeed.\n","\n","GraphModule(\n","  117 = 100% Params, 640 MACs = 100% MACs, 689.51 us = 100% latency, 2.78 MFLOPS\n","  (seq_blocks): Module(\n","    117 = 100% Params, 640 MACs = 100% MACs, 573.16 us = 83.13% latency, 3.34 MFLOPS\n","    (0): BatchNorm1d(32 = 27.35% Params, 0 MACs = 0% MACs, 185.25 us = 26.87% latency, 1.38 MFLOPS, 16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","    (1): ReLU(0 = 0% Params, 0 MACs = 0% MACs, 48.16 us = 6.98% latency, 2.66 MFLOPS, inplace=True)\n","    (2): LinearInteger(85 = 72.65% Params, 640 MACs = 100% MACs, 301.6 us = 43.74% latency, 4.95 MFLOPS, in_features=16, out_features=5, bias=True)\n","    (3): ReLU(0 = 0% Params, 0 MACs = 0% MACs, 38.15 us = 5.53% latency, 1.05 MFLOPS, inplace=True)\n","  )\n",")\n","\n","\n","\n","def forward(self, x):\n","    seq_blocks_0 = getattr(self.seq_blocks, \"0\")(x);  x = None\n","    seq_blocks_1 = getattr(self.seq_blocks, \"1\")(seq_blocks_0);  seq_blocks_0 = None\n","    seq_blocks_2 = getattr(self.seq_blocks, \"2\")(seq_blocks_1);  seq_blocks_1 = None\n","    seq_blocks_3 = getattr(self.seq_blocks, \"3\")(seq_blocks_2);  seq_blocks_2 = None\n","    return seq_blocks_3\n","    \n","# To see more debug info, please use `graph_module.print_readable()`\n","------------------------------------------------------------------------------\n","[2024-02-11 16:16:23,528] [INFO] [profiler.py:226:end_profile] Flops profiler finished\n","torch.Size([8, 16])\n","[2024-02-11 16:16:23,571] [INFO] [profiler.py:1205:get_model_profile] Flops profiler warming-up...\n","[2024-02-11 16:16:23,572] [INFO] [profiler.py:80:start_profile] Flops profiler started\n","\n","-------------------------- DeepSpeed Flops Profiler --------------------------\n","Profile Summary at step 1:\n","Notations:\n","data parallel size (dp_size), model parallel size(mp_size),\n","number of parameters (params), number of multiply-accumulate operations(MACs),\n","number of floating-point operations (flops), floating-point operations per second (FLOPS),\n","fwd latency (forward propagation latency), bwd latency (backward propagation latency),\n","step (weights update latency), iter latency (sum of fwd, bwd and step latency)\n","\n","params per GPU:                                                         117     \n","params of model = params per GPU * mp_size:                             0       \n","fwd MACs per GPU:                                                       640 MACs\n","fwd flops per GPU:                                                      1.92 K  \n","fwd flops of model = fwd flops per GPU * mp_size:                       1.92 K  \n","fwd latency:                                                            542.88 us\n","fwd FLOPS per GPU = fwd flops per GPU / fwd latency:                    3.53 MFLOPS\n","\n","----------------------------- Aggregated Profile per GPU -----------------------------\n","Top 1 modules in terms of params, MACs or fwd latency at different model depths:\n","depth 0:\n","    params      - {'GraphModule': '117'}\n","    MACs        - {'GraphModule': '640 MACs'}\n","    fwd latency - {'GraphModule': '542.88 us'}\n","depth 1:\n","    params      - {'Module': '117'}\n","    MACs        - {'Module': '640 MACs'}\n","    fwd latency - {'Module': '444.89 us'}\n","\n","------------------------------ Detailed Profile per GPU ------------------------------\n","Each module profile is listed after its name in the following order: \n","params, percentage of total params, MACs, percentage of total MACs, fwd latency, percentage of total fwd latency, fwd FLOPS\n","\n","Note: 1. A module can have torch.nn.module or torch.nn.functional to compute logits (e.g. CrossEntropyLoss). They are not counted as submodules, thus not to be printed out. However they make up the difference between a parent's MACs (or latency) and the sum of its submodules'.\n","2. Number of floating-point operations is a theoretical estimation, thus FLOPS computed using that could be larger than the maximum system throughput.\n","3. The fwd latency listed in the top module's profile is directly captured at the module forward function in PyTorch, thus it's less than the fwd latency shown above which is captured in DeepSpeed.\n","\n","GraphModule(\n","  117 = 100% Params, 640 MACs = 100% MACs, 542.88 us = 100% latency, 3.53 MFLOPS\n","  (seq_blocks): Module(\n","    117 = 100% Params, 640 MACs = 100% MACs, 444.89 us = 81.95% latency, 4.31 MFLOPS\n","    (0): BatchNorm1d(32 = 27.35% Params, 0 MACs = 0% MACs, 139.24 us = 25.65% latency, 1.84 MFLOPS, 16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","    (1): ReLU(0 = 0% Params, 0 MACs = 0% MACs, 35.05 us = 6.46% latency, 3.65 MFLOPS, inplace=True)\n","    (2): LinearInteger(85 = 72.65% Params, 640 MACs = 100% MACs, 236.99 us = 43.65% latency, 6.3 MFLOPS, in_features=16, out_features=5, bias=True)\n","    (3): ReLU(0 = 0% Params, 0 MACs = 0% MACs, 33.62 us = 6.19% latency, 1.19 MFLOPS, inplace=True)\n","  )\n",")\n","\n","\n","\n","def forward(self, x):\n","    seq_blocks_0 = getattr(self.seq_blocks, \"0\")(x);  x = None\n","    seq_blocks_1 = getattr(self.seq_blocks, \"1\")(seq_blocks_0);  seq_blocks_0 = None\n","    seq_blocks_2 = getattr(self.seq_blocks, \"2\")(seq_blocks_1);  seq_blocks_1 = None\n","    seq_blocks_3 = getattr(self.seq_blocks, \"3\")(seq_blocks_2);  seq_blocks_2 = None\n","    return seq_blocks_3\n","    \n","# To see more debug info, please use `graph_module.print_readable()`\n","------------------------------------------------------------------------------\n","[2024-02-11 16:16:23,574] [INFO] [profiler.py:226:end_profile] Flops profiler finished\n","torch.Size([8, 16])\n","[2024-02-11 16:16:23,621] [INFO] [profiler.py:1205:get_model_profile] Flops profiler warming-up...\n","[2024-02-11 16:16:23,622] [INFO] [profiler.py:80:start_profile] Flops profiler started\n","\n","-------------------------- DeepSpeed Flops Profiler --------------------------\n","Profile Summary at step 1:\n","Notations:\n","data parallel size (dp_size), model parallel size(mp_size),\n","number of parameters (params), number of multiply-accumulate operations(MACs),\n","number of floating-point operations (flops), floating-point operations per second (FLOPS),\n","fwd latency (forward propagation latency), bwd latency (backward propagation latency),\n","step (weights update latency), iter latency (sum of fwd, bwd and step latency)\n","\n","params per GPU:                                                         117     \n","params of model = params per GPU * mp_size:                             0       \n","fwd MACs per GPU:                                                       640 MACs\n","fwd flops per GPU:                                                      1.92 K  \n","fwd flops of model = fwd flops per GPU * mp_size:                       1.92 K  \n","fwd latency:                                                            701.19 us\n","fwd FLOPS per GPU = fwd flops per GPU / fwd latency:                    2.73 MFLOPS\n","\n","----------------------------- Aggregated Profile per GPU -----------------------------\n","Top 1 modules in terms of params, MACs or fwd latency at different model depths:\n","depth 0:\n","    params      - {'GraphModule': '117'}\n","    MACs        - {'GraphModule': '640 MACs'}\n","    fwd latency - {'GraphModule': '701.19 us'}\n","depth 1:\n","    params      - {'Module': '117'}\n","    MACs        - {'Module': '640 MACs'}\n","    fwd latency - {'Module': '557.66 us'}\n","\n","------------------------------ Detailed Profile per GPU ------------------------------\n","Each module profile is listed after its name in the following order: \n","params, percentage of total params, MACs, percentage of total MACs, fwd latency, percentage of total fwd latency, fwd FLOPS\n","\n","Note: 1. A module can have torch.nn.module or torch.nn.functional to compute logits (e.g. CrossEntropyLoss). They are not counted as submodules, thus not to be printed out. However they make up the difference between a parent's MACs (or latency) and the sum of its submodules'.\n","2. Number of floating-point operations is a theoretical estimation, thus FLOPS computed using that could be larger than the maximum system throughput.\n","3. The fwd latency listed in the top module's profile is directly captured at the module forward function in PyTorch, thus it's less than the fwd latency shown above which is captured in DeepSpeed.\n","\n","GraphModule(\n","  117 = 100% Params, 640 MACs = 100% MACs, 701.19 us = 100% latency, 2.73 MFLOPS\n","  (seq_blocks): Module(\n","    117 = 100% Params, 640 MACs = 100% MACs, 557.66 us = 79.53% latency, 3.44 MFLOPS\n","    (0): BatchNorm1d(32 = 27.35% Params, 0 MACs = 0% MACs, 99.9 us = 14.25% latency, 2.56 MFLOPS, 16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","    (1): ReLU(0 = 0% Params, 0 MACs = 0% MACs, 51.98 us = 7.41% latency, 2.46 MFLOPS, inplace=True)\n","    (2): LinearInteger(85 = 72.65% Params, 640 MACs = 100% MACs, 357.63 us = 51% latency, 4.17 MFLOPS, in_features=16, out_features=5, bias=True)\n","    (3): ReLU(0 = 0% Params, 0 MACs = 0% MACs, 48.16 us = 6.87% latency, 830.56 KFLOPS, inplace=True)\n","  )\n",")\n","\n","\n","\n","def forward(self, x):\n","    seq_blocks_0 = getattr(self.seq_blocks, \"0\")(x);  x = None\n","    seq_blocks_1 = getattr(self.seq_blocks, \"1\")(seq_blocks_0);  seq_blocks_0 = None\n","    seq_blocks_2 = getattr(self.seq_blocks, \"2\")(seq_blocks_1);  seq_blocks_1 = None\n","    seq_blocks_3 = getattr(self.seq_blocks, \"3\")(seq_blocks_2);  seq_blocks_2 = None\n","    return seq_blocks_3\n","    \n","# To see more debug info, please use `graph_module.print_readable()`\n","------------------------------------------------------------------------------\n","[2024-02-11 16:16:23,625] [INFO] [profiler.py:226:end_profile] Flops profiler finished\n","torch.Size([8, 16])\n","[2024-02-11 16:16:23,677] [INFO] [profiler.py:1205:get_model_profile] Flops profiler warming-up...\n","[2024-02-11 16:16:23,678] [INFO] [profiler.py:80:start_profile] Flops profiler started\n","\n","-------------------------- DeepSpeed Flops Profiler --------------------------\n","Profile Summary at step 1:\n","Notations:\n","data parallel size (dp_size), model parallel size(mp_size),\n","number of parameters (params), number of multiply-accumulate operations(MACs),\n","number of floating-point operations (flops), floating-point operations per second (FLOPS),\n","fwd latency (forward propagation latency), bwd latency (backward propagation latency),\n","step (weights update latency), iter latency (sum of fwd, bwd and step latency)\n","\n","params per GPU:                                                         117     \n","params of model = params per GPU * mp_size:                             0       \n","fwd MACs per GPU:                                                       640 MACs\n","fwd flops per GPU:                                                      1.92 K  \n","fwd flops of model = fwd flops per GPU * mp_size:                       1.92 K  \n","fwd latency:                                                            673.29 us\n","fwd FLOPS per GPU = fwd flops per GPU / fwd latency:                    2.85 MFLOPS\n","\n","----------------------------- Aggregated Profile per GPU -----------------------------\n","Top 1 modules in terms of params, MACs or fwd latency at different model depths:\n","depth 0:\n","    params      - {'GraphModule': '117'}\n","    MACs        - {'GraphModule': '640 MACs'}\n","    fwd latency - {'GraphModule': '673.29 us'}\n","depth 1:\n","    params      - {'Module': '117'}\n","    MACs        - {'Module': '640 MACs'}\n","    fwd latency - {'Module': '531.44 us'}\n","\n","------------------------------ Detailed Profile per GPU ------------------------------\n","Each module profile is listed after its name in the following order: \n","params, percentage of total params, MACs, percentage of total MACs, fwd latency, percentage of total fwd latency, fwd FLOPS\n","\n","Note: 1. A module can have torch.nn.module or torch.nn.functional to compute logits (e.g. CrossEntropyLoss). They are not counted as submodules, thus not to be printed out. However they make up the difference between a parent's MACs (or latency) and the sum of its submodules'.\n","2. Number of floating-point operations is a theoretical estimation, thus FLOPS computed using that could be larger than the maximum system throughput.\n","3. The fwd latency listed in the top module's profile is directly captured at the module forward function in PyTorch, thus it's less than the fwd latency shown above which is captured in DeepSpeed.\n","\n","GraphModule(\n","  117 = 100% Params, 640 MACs = 100% MACs, 673.29 us = 100% latency, 2.85 MFLOPS\n","  (seq_blocks): Module(\n","    117 = 100% Params, 640 MACs = 100% MACs, 531.44 us = 78.93% latency, 3.61 MFLOPS\n","    (0): BatchNorm1d(32 = 27.35% Params, 0 MACs = 0% MACs, 105.38 us = 15.65% latency, 2.43 MFLOPS, 16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","    (1): ReLU(0 = 0% Params, 0 MACs = 0% MACs, 62.7 us = 9.31% latency, 2.04 MFLOPS, inplace=True)\n","    (2): LinearInteger(85 = 72.65% Params, 640 MACs = 100% MACs, 328.54 us = 48.8% latency, 4.54 MFLOPS, in_features=16, out_features=5, bias=True)\n","    (3): ReLU(0 = 0% Params, 0 MACs = 0% MACs, 34.81 us = 5.17% latency, 1.15 MFLOPS, inplace=True)\n","  )\n",")\n","\n","\n","\n","def forward(self, x):\n","    seq_blocks_0 = getattr(self.seq_blocks, \"0\")(x);  x = None\n","    seq_blocks_1 = getattr(self.seq_blocks, \"1\")(seq_blocks_0);  seq_blocks_0 = None\n","    seq_blocks_2 = getattr(self.seq_blocks, \"2\")(seq_blocks_1);  seq_blocks_1 = None\n","    seq_blocks_3 = getattr(self.seq_blocks, \"3\")(seq_blocks_2);  seq_blocks_2 = None\n","    return seq_blocks_3\n","    \n","# To see more debug info, please use `graph_module.print_readable()`\n","------------------------------------------------------------------------------\n","[2024-02-11 16:16:23,681] [INFO] [profiler.py:226:end_profile] Flops profiler finished\n","torch.Size([8, 16])\n","[2024-02-11 16:16:23,727] [INFO] [profiler.py:1205:get_model_profile] Flops profiler warming-up...\n","[2024-02-11 16:16:23,728] [INFO] [profiler.py:80:start_profile] Flops profiler started\n","\n","-------------------------- DeepSpeed Flops Profiler --------------------------\n","Profile Summary at step 1:\n","Notations:\n","data parallel size (dp_size), model parallel size(mp_size),\n","number of parameters (params), number of multiply-accumulate operations(MACs),\n","number of floating-point operations (flops), floating-point operations per second (FLOPS),\n","fwd latency (forward propagation latency), bwd latency (backward propagation latency),\n","step (weights update latency), iter latency (sum of fwd, bwd and step latency)\n","\n","params per GPU:                                                         117     \n","params of model = params per GPU * mp_size:                             0       \n","fwd MACs per GPU:                                                       640 MACs\n","fwd flops per GPU:                                                      1.92 K  \n","fwd flops of model = fwd flops per GPU * mp_size:                       1.92 K  \n","fwd latency:                                                            988.72 us\n","fwd FLOPS per GPU = fwd flops per GPU / fwd latency:                    1.94 MFLOPS\n","\n","----------------------------- Aggregated Profile per GPU -----------------------------\n","Top 1 modules in terms of params, MACs or fwd latency at different model depths:\n","depth 0:\n","    params      - {'GraphModule': '117'}\n","    MACs        - {'GraphModule': '640 MACs'}\n","    fwd latency - {'GraphModule': '988.72 us'}\n","depth 1:\n","    params      - {'Module': '117'}\n","    MACs        - {'Module': '640 MACs'}\n","    fwd latency - {'Module': '817.06 us'}\n","\n","------------------------------ Detailed Profile per GPU ------------------------------\n","Each module profile is listed after its name in the following order: \n","params, percentage of total params, MACs, percentage of total MACs, fwd latency, percentage of total fwd latency, fwd FLOPS\n","\n","Note: 1. A module can have torch.nn.module or torch.nn.functional to compute logits (e.g. CrossEntropyLoss). They are not counted as submodules, thus not to be printed out. However they make up the difference between a parent's MACs (or latency) and the sum of its submodules'.\n","2. Number of floating-point operations is a theoretical estimation, thus FLOPS computed using that could be larger than the maximum system throughput.\n","3. The fwd latency listed in the top module's profile is directly captured at the module forward function in PyTorch, thus it's less than the fwd latency shown above which is captured in DeepSpeed.\n","\n","GraphModule(\n","  117 = 100% Params, 640 MACs = 100% MACs, 988.72 us = 100% latency, 1.94 MFLOPS\n","  (seq_blocks): Module(\n","    117 = 100% Params, 640 MACs = 100% MACs, 817.06 us = 82.64% latency, 2.35 MFLOPS\n","    (0): BatchNorm1d(32 = 27.35% Params, 0 MACs = 0% MACs, 260.35 us = 26.33% latency, 983.28 KFLOPS, 16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","    (1): ReLU(0 = 0% Params, 0 MACs = 0% MACs, 67.47 us = 6.82% latency, 1.9 MFLOPS, inplace=True)\n","    (2): LinearInteger(85 = 72.65% Params, 640 MACs = 100% MACs, 430.11 us = 43.5% latency, 3.47 MFLOPS, in_features=16, out_features=5, bias=True)\n","    (3): ReLU(0 = 0% Params, 0 MACs = 0% MACs, 59.13 us = 5.98% latency, 676.5 KFLOPS, inplace=True)\n","  )\n",")\n","\n","\n","\n","def forward(self, x):\n","    seq_blocks_0 = getattr(self.seq_blocks, \"0\")(x);  x = None\n","    seq_blocks_1 = getattr(self.seq_blocks, \"1\")(seq_blocks_0);  seq_blocks_0 = None\n","    seq_blocks_2 = getattr(self.seq_blocks, \"2\")(seq_blocks_1);  seq_blocks_1 = None\n","    seq_blocks_3 = getattr(self.seq_blocks, \"3\")(seq_blocks_2);  seq_blocks_2 = None\n","    return seq_blocks_3\n","    \n","# To see more debug info, please use `graph_module.print_readable()`\n","------------------------------------------------------------------------------\n","[2024-02-11 16:16:23,731] [INFO] [profiler.py:226:end_profile] Flops profiler finished\n","torch.Size([8, 16])\n","[2024-02-11 16:16:23,768] [INFO] [profiler.py:1205:get_model_profile] Flops profiler warming-up...\n","[2024-02-11 16:16:23,769] [INFO] [profiler.py:80:start_profile] Flops profiler started\n","\n","-------------------------- DeepSpeed Flops Profiler --------------------------\n","Profile Summary at step 1:\n","Notations:\n","data parallel size (dp_size), model parallel size(mp_size),\n","number of parameters (params), number of multiply-accumulate operations(MACs),\n","number of floating-point operations (flops), floating-point operations per second (FLOPS),\n","fwd latency (forward propagation latency), bwd latency (backward propagation latency),\n","step (weights update latency), iter latency (sum of fwd, bwd and step latency)\n","\n","params per GPU:                                                         117     \n","params of model = params per GPU * mp_size:                             0       \n","fwd MACs per GPU:                                                       640 MACs\n","fwd flops per GPU:                                                      1.92 K  \n","fwd flops of model = fwd flops per GPU * mp_size:                       1.92 K  \n","fwd latency:                                                            851.87 us\n","fwd FLOPS per GPU = fwd flops per GPU / fwd latency:                    2.25 MFLOPS\n","\n","----------------------------- Aggregated Profile per GPU -----------------------------\n","Top 1 modules in terms of params, MACs or fwd latency at different model depths:\n","depth 0:\n","    params      - {'GraphModule': '117'}\n","    MACs        - {'GraphModule': '640 MACs'}\n","    fwd latency - {'GraphModule': '851.87 us'}\n","depth 1:\n","    params      - {'Module': '117'}\n","    MACs        - {'Module': '640 MACs'}\n","    fwd latency - {'Module': '702.14 us'}\n","\n","------------------------------ Detailed Profile per GPU ------------------------------\n","Each module profile is listed after its name in the following order: \n","params, percentage of total params, MACs, percentage of total MACs, fwd latency, percentage of total fwd latency, fwd FLOPS\n","\n","Note: 1. A module can have torch.nn.module or torch.nn.functional to compute logits (e.g. CrossEntropyLoss). They are not counted as submodules, thus not to be printed out. However they make up the difference between a parent's MACs (or latency) and the sum of its submodules'.\n","2. Number of floating-point operations is a theoretical estimation, thus FLOPS computed using that could be larger than the maximum system throughput.\n","3. The fwd latency listed in the top module's profile is directly captured at the module forward function in PyTorch, thus it's less than the fwd latency shown above which is captured in DeepSpeed.\n","\n","GraphModule(\n","  117 = 100% Params, 640 MACs = 100% MACs, 851.87 us = 100% latency, 2.25 MFLOPS\n","  (seq_blocks): Module(\n","    117 = 100% Params, 640 MACs = 100% MACs, 702.14 us = 82.42% latency, 2.73 MFLOPS\n","    (0): BatchNorm1d(32 = 27.35% Params, 0 MACs = 0% MACs, 191.93 us = 22.53% latency, 1.33 MFLOPS, 16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","    (1): ReLU(0 = 0% Params, 0 MACs = 0% MACs, 60.08 us = 7.05% latency, 2.13 MFLOPS, inplace=True)\n","    (2): LinearInteger(85 = 72.65% Params, 640 MACs = 100% MACs, 414.61 us = 48.67% latency, 3.6 MFLOPS, in_features=16, out_features=5, bias=True)\n","    (3): ReLU(0 = 0% Params, 0 MACs = 0% MACs, 35.52 us = 4.17% latency, 1.13 MFLOPS, inplace=True)\n","  )\n",")\n","\n","\n","\n","def forward(self, x):\n","    seq_blocks_0 = getattr(self.seq_blocks, \"0\")(x);  x = None\n","    seq_blocks_1 = getattr(self.seq_blocks, \"1\")(seq_blocks_0);  seq_blocks_0 = None\n","    seq_blocks_2 = getattr(self.seq_blocks, \"2\")(seq_blocks_1);  seq_blocks_1 = None\n","    seq_blocks_3 = getattr(self.seq_blocks, \"3\")(seq_blocks_2);  seq_blocks_2 = None\n","    return seq_blocks_3\n","    \n","# To see more debug info, please use `graph_module.print_readable()`\n","------------------------------------------------------------------------------\n","[2024-02-11 16:16:23,772] [INFO] [profiler.py:226:end_profile] Flops profiler finished\n","torch.Size([8, 16])\n","[2024-02-11 16:16:23,809] [INFO] [profiler.py:1205:get_model_profile] Flops profiler warming-up...\n","[2024-02-11 16:16:23,810] [INFO] [profiler.py:80:start_profile] Flops profiler started\n","\n","-------------------------- DeepSpeed Flops Profiler --------------------------\n","Profile Summary at step 1:\n","Notations:\n","data parallel size (dp_size), model parallel size(mp_size),\n","number of parameters (params), number of multiply-accumulate operations(MACs),\n","number of floating-point operations (flops), floating-point operations per second (FLOPS),\n","fwd latency (forward propagation latency), bwd latency (backward propagation latency),\n","step (weights update latency), iter latency (sum of fwd, bwd and step latency)\n","\n","params per GPU:                                                         117     \n","params of model = params per GPU * mp_size:                             0       \n","fwd MACs per GPU:                                                       640 MACs\n","fwd flops per GPU:                                                      1.92 K  \n","fwd flops of model = fwd flops per GPU * mp_size:                       1.92 K  \n","fwd latency:                                                            735.52 us\n","fwd FLOPS per GPU = fwd flops per GPU / fwd latency:                    2.61 MFLOPS\n","\n","----------------------------- Aggregated Profile per GPU -----------------------------\n","Top 1 modules in terms of params, MACs or fwd latency at different model depths:\n","depth 0:\n","    params      - {'GraphModule': '117'}\n","    MACs        - {'GraphModule': '640 MACs'}\n","    fwd latency - {'GraphModule': '735.52 us'}\n","depth 1:\n","    params      - {'Module': '117'}\n","    MACs        - {'Module': '640 MACs'}\n","    fwd latency - {'Module': '598.43 us'}\n","\n","------------------------------ Detailed Profile per GPU ------------------------------\n","Each module profile is listed after its name in the following order: \n","params, percentage of total params, MACs, percentage of total MACs, fwd latency, percentage of total fwd latency, fwd FLOPS\n","\n","Note: 1. A module can have torch.nn.module or torch.nn.functional to compute logits (e.g. CrossEntropyLoss). They are not counted as submodules, thus not to be printed out. However they make up the difference between a parent's MACs (or latency) and the sum of its submodules'.\n","2. Number of floating-point operations is a theoretical estimation, thus FLOPS computed using that could be larger than the maximum system throughput.\n","3. The fwd latency listed in the top module's profile is directly captured at the module forward function in PyTorch, thus it's less than the fwd latency shown above which is captured in DeepSpeed.\n","\n","GraphModule(\n","  117 = 100% Params, 640 MACs = 100% MACs, 735.52 us = 100% latency, 2.61 MFLOPS\n","  (seq_blocks): Module(\n","    117 = 100% Params, 640 MACs = 100% MACs, 598.43 us = 81.36% latency, 3.2 MFLOPS\n","    (0): BatchNorm1d(32 = 27.35% Params, 0 MACs = 0% MACs, 221.97 us = 30.18% latency, 1.15 MFLOPS, 16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","    (1): ReLU(0 = 0% Params, 0 MACs = 0% MACs, 50.54 us = 6.87% latency, 2.53 MFLOPS, inplace=True)\n","    (2): LinearInteger(85 = 72.65% Params, 640 MACs = 100% MACs, 287.29 us = 39.06% latency, 5.2 MFLOPS, in_features=16, out_features=5, bias=True)\n","    (3): ReLU(0 = 0% Params, 0 MACs = 0% MACs, 38.62 us = 5.25% latency, 1.04 MFLOPS, inplace=True)\n","  )\n",")\n","\n","\n","\n","def forward(self, x):\n","    seq_blocks_0 = getattr(self.seq_blocks, \"0\")(x);  x = None\n","    seq_blocks_1 = getattr(self.seq_blocks, \"1\")(seq_blocks_0);  seq_blocks_0 = None\n","    seq_blocks_2 = getattr(self.seq_blocks, \"2\")(seq_blocks_1);  seq_blocks_1 = None\n","    seq_blocks_3 = getattr(self.seq_blocks, \"3\")(seq_blocks_2);  seq_blocks_2 = None\n","    return seq_blocks_3\n","    \n","# To see more debug info, please use `graph_module.print_readable()`\n","------------------------------------------------------------------------------\n","[2024-02-11 16:16:23,812] [INFO] [profiler.py:226:end_profile] Flops profiler finished\n","torch.Size([8, 16])\n","[2024-02-11 16:16:23,855] [INFO] [profiler.py:1205:get_model_profile] Flops profiler warming-up...\n","[2024-02-11 16:16:23,856] [INFO] [profiler.py:80:start_profile] Flops profiler started\n","\n","-------------------------- DeepSpeed Flops Profiler --------------------------\n","Profile Summary at step 1:\n","Notations:\n","data parallel size (dp_size), model parallel size(mp_size),\n","number of parameters (params), number of multiply-accumulate operations(MACs),\n","number of floating-point operations (flops), floating-point operations per second (FLOPS),\n","fwd latency (forward propagation latency), bwd latency (backward propagation latency),\n","step (weights update latency), iter latency (sum of fwd, bwd and step latency)\n","\n","params per GPU:                                                         117     \n","params of model = params per GPU * mp_size:                             0       \n","fwd MACs per GPU:                                                       640 MACs\n","fwd flops per GPU:                                                      1.92 K  \n","fwd flops of model = fwd flops per GPU * mp_size:                       1.92 K  \n","fwd latency:                                                            622.75 us\n","fwd FLOPS per GPU = fwd flops per GPU / fwd latency:                    3.08 MFLOPS\n","\n","----------------------------- Aggregated Profile per GPU -----------------------------\n","Top 1 modules in terms of params, MACs or fwd latency at different model depths:\n","depth 0:\n","    params      - {'GraphModule': '117'}\n","    MACs        - {'GraphModule': '640 MACs'}\n","    fwd latency - {'GraphModule': '622.75 us'}\n","depth 1:\n","    params      - {'Module': '117'}\n","    MACs        - {'Module': '640 MACs'}\n","    fwd latency - {'Module': '511.41 us'}\n","\n","------------------------------ Detailed Profile per GPU ------------------------------\n","Each module profile is listed after its name in the following order: \n","params, percentage of total params, MACs, percentage of total MACs, fwd latency, percentage of total fwd latency, fwd FLOPS\n","\n","Note: 1. A module can have torch.nn.module or torch.nn.functional to compute logits (e.g. CrossEntropyLoss). They are not counted as submodules, thus not to be printed out. However they make up the difference between a parent's MACs (or latency) and the sum of its submodules'.\n","2. Number of floating-point operations is a theoretical estimation, thus FLOPS computed using that could be larger than the maximum system throughput.\n","3. The fwd latency listed in the top module's profile is directly captured at the module forward function in PyTorch, thus it's less than the fwd latency shown above which is captured in DeepSpeed.\n","\n","GraphModule(\n","  117 = 100% Params, 640 MACs = 100% MACs, 622.75 us = 100% latency, 3.08 MFLOPS\n","  (seq_blocks): Module(\n","    117 = 100% Params, 640 MACs = 100% MACs, 511.41 us = 82.12% latency, 3.75 MFLOPS\n","    (0): BatchNorm1d(32 = 27.35% Params, 0 MACs = 0% MACs, 180.48 us = 28.98% latency, 1.42 MFLOPS, 16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","    (1): ReLU(0 = 0% Params, 0 MACs = 0% MACs, 40.05 us = 6.43% latency, 3.2 MFLOPS, inplace=True)\n","    (2): LinearInteger(85 = 72.65% Params, 640 MACs = 100% MACs, 253.92 us = 40.77% latency, 5.88 MFLOPS, in_features=16, out_features=5, bias=True)\n","    (3): ReLU(0 = 0% Params, 0 MACs = 0% MACs, 36.95 us = 5.93% latency, 1.08 MFLOPS, inplace=True)\n","  )\n",")\n","\n","\n","\n","def forward(self, x):\n","    seq_blocks_0 = getattr(self.seq_blocks, \"0\")(x);  x = None\n","    seq_blocks_1 = getattr(self.seq_blocks, \"1\")(seq_blocks_0);  seq_blocks_0 = None\n","    seq_blocks_2 = getattr(self.seq_blocks, \"2\")(seq_blocks_1);  seq_blocks_1 = None\n","    seq_blocks_3 = getattr(self.seq_blocks, \"3\")(seq_blocks_2);  seq_blocks_2 = None\n","    return seq_blocks_3\n","    \n","# To see more debug info, please use `graph_module.print_readable()`\n","------------------------------------------------------------------------------\n","[2024-02-11 16:16:23,858] [INFO] [profiler.py:226:end_profile] Flops profiler finished\n","torch.Size([8, 16])\n","[2024-02-11 16:16:23,901] [INFO] [profiler.py:1205:get_model_profile] Flops profiler warming-up...\n","[2024-02-11 16:16:23,902] [INFO] [profiler.py:80:start_profile] Flops profiler started\n","\n","-------------------------- DeepSpeed Flops Profiler --------------------------\n","Profile Summary at step 1:\n","Notations:\n","data parallel size (dp_size), model parallel size(mp_size),\n","number of parameters (params), number of multiply-accumulate operations(MACs),\n","number of floating-point operations (flops), floating-point operations per second (FLOPS),\n","fwd latency (forward propagation latency), bwd latency (backward propagation latency),\n","step (weights update latency), iter latency (sum of fwd, bwd and step latency)\n","\n","params per GPU:                                                         117     \n","params of model = params per GPU * mp_size:                             0       \n","fwd MACs per GPU:                                                       640 MACs\n","fwd flops per GPU:                                                      1.92 K  \n","fwd flops of model = fwd flops per GPU * mp_size:                       1.92 K  \n","fwd latency:                                                            599.86 us\n","fwd FLOPS per GPU = fwd flops per GPU / fwd latency:                    3.2 MFLOPS\n","\n","----------------------------- Aggregated Profile per GPU -----------------------------\n","Top 1 modules in terms of params, MACs or fwd latency at different model depths:\n","depth 0:\n","    params      - {'GraphModule': '117'}\n","    MACs        - {'GraphModule': '640 MACs'}\n","    fwd latency - {'GraphModule': '599.86 us'}\n","depth 1:\n","    params      - {'Module': '117'}\n","    MACs        - {'Module': '640 MACs'}\n","    fwd latency - {'Module': '490.19 us'}\n","\n","------------------------------ Detailed Profile per GPU ------------------------------\n","Each module profile is listed after its name in the following order: \n","params, percentage of total params, MACs, percentage of total MACs, fwd latency, percentage of total fwd latency, fwd FLOPS\n","\n","Note: 1. A module can have torch.nn.module or torch.nn.functional to compute logits (e.g. CrossEntropyLoss). They are not counted as submodules, thus not to be printed out. However they make up the difference between a parent's MACs (or latency) and the sum of its submodules'.\n","2. Number of floating-point operations is a theoretical estimation, thus FLOPS computed using that could be larger than the maximum system throughput.\n","3. The fwd latency listed in the top module's profile is directly captured at the module forward function in PyTorch, thus it's less than the fwd latency shown above which is captured in DeepSpeed.\n","\n","GraphModule(\n","  117 = 100% Params, 640 MACs = 100% MACs, 599.86 us = 100% latency, 3.2 MFLOPS\n","  (seq_blocks): Module(\n","    117 = 100% Params, 640 MACs = 100% MACs, 490.19 us = 81.72% latency, 3.91 MFLOPS\n","    (0): BatchNorm1d(32 = 27.35% Params, 0 MACs = 0% MACs, 167.61 us = 27.94% latency, 1.53 MFLOPS, 16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","    (1): ReLU(0 = 0% Params, 0 MACs = 0% MACs, 37.91 us = 6.32% latency, 3.38 MFLOPS, inplace=True)\n","    (2): LinearInteger(85 = 72.65% Params, 640 MACs = 100% MACs, 248.19 us = 41.38% latency, 6.02 MFLOPS, in_features=16, out_features=5, bias=True)\n","    (3): ReLU(0 = 0% Params, 0 MACs = 0% MACs, 36.48 us = 6.08% latency, 1.1 MFLOPS, inplace=True)\n","  )\n",")\n","\n","\n","\n","def forward(self, x):\n","    seq_blocks_0 = getattr(self.seq_blocks, \"0\")(x);  x = None\n","    seq_blocks_1 = getattr(self.seq_blocks, \"1\")(seq_blocks_0);  seq_blocks_0 = None\n","    seq_blocks_2 = getattr(self.seq_blocks, \"2\")(seq_blocks_1);  seq_blocks_1 = None\n","    seq_blocks_3 = getattr(self.seq_blocks, \"3\")(seq_blocks_2);  seq_blocks_2 = None\n","    return seq_blocks_3\n","    \n","# To see more debug info, please use `graph_module.print_readable()`\n","------------------------------------------------------------------------------\n","[2024-02-11 16:16:23,904] [INFO] [profiler.py:226:end_profile] Flops profiler finished\n"]}],"source":["metric = MulticlassAccuracy(num_classes=5)\n","#get_model_profile\n","num_batchs = 5\n","# This first loop is basically our search strategy,\n","# in this case, it is a simple brute force search\n","recorded_accs = []\n","recorded_loss = []\n","recorded_latencies = []\n","recorded_model_sizes = []\n","recorded_flops = []\n","for i, config in enumerate(search_spaces):\n","    # print(i)\n","    mg, _ = quantize_transform_pass(mg, config)\n","    j = 0\n","    print((dummy_in['x'].shape))\n","    flops, macs, params = get_model_profile(model=mg.model, input_shape=tuple(dummy_in['x'].shape))\n","    recorded_model_sizes.append(params)\n","    recorded_flops.append(flops)\n","\n","    # this is the inner loop, where we also call it as a runner.\n","    acc_avg, loss_avg = 0, 0\n","    accs, losses = [], []\n","    \n","    # Additional code for measuring latency\n","    start_time = torch.cuda.Event(enable_timing=True)\n","    end_time = torch.cuda.Event(enable_timing=True)\n","\n","    for inputs in data_module.train_dataloader():\n","        xs, ys = inputs\n","        preds = mg.model(xs)\n","        loss = torch.nn.functional.cross_entropy(preds, ys)\n","        acc = np.array(metric(preds, ys))\n","        accs.append(acc)\n","        losses.append(loss)\n","        if j == 0:\n","            start_time.record()\n","        elif j == num_batchs:\n","            end_time.record()\n","            torch.cuda.synchronize()  # Wait for all GPU operations to finish\n","            latency = start_time.elapsed_time(end_time) / num_batchs\n","            recorded_latencies.append(latency)\n","        if j > num_batchs:\n","            break\n","        j += 1\n","    acc_avg = sum(accs) / len(accs)\n","    loss_avg = sum(losses) / len(losses)\n","    recorded_accs.append(acc_avg)\n","    recorded_loss.append(loss_avg)\n","\n","\n","#     recorded_accs = []\n","# recorded_latencies = []\n","# recorded_model_sizes = []\n","# recorded_flops = []\n"]},{"cell_type":"markdown","metadata":{},"source":["2. Implement some of these additional metrics and attempt to combine them with the accuracy or loss quality metric. It's important to note that in this particular case, accuracy and loss actually serve as the same quality metric (do you know why?).\n","\n"]},{"cell_type":"code","execution_count":25,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["[[(16, 8), (16, 8)], [(16, 8), (8, 6)], [(16, 8), (8, 4)], [(16, 8), (4, 2)], [(8, 6), (16, 8)], [(8, 6), (8, 6)], [(8, 6), (8, 4)], [(8, 6), (4, 2)], [(8, 4), (16, 8)], [(8, 4), (8, 6)], [(8, 4), (8, 4)], [(8, 4), (4, 2)], [(4, 2), (16, 8)], [(4, 2), (8, 6)], [(4, 2), (8, 4)], [(4, 2), (4, 2)]]\n","[0.47261905670166016, 0.31607143793787273, 0.3761904835700989, 0.34523810765572954, 0.4285714456013271, 0.39047619913305553, 0.4464285799435207, 0.44523810488837107, 0.46666668142591206, 0.5333333377327237, 0.5234127044677734, 0.3839285799435207, 0.5160714387893677, 0.5154761991330555, 0.5148809594767434, 0.4904761953013284]\n","[tensor(1.2069, grad_fn=<DivBackward0>), tensor(1.4612, grad_fn=<DivBackward0>), tensor(1.4007, grad_fn=<DivBackward0>), tensor(1.4049, grad_fn=<DivBackward0>), tensor(1.3946, grad_fn=<DivBackward0>), tensor(1.3525, grad_fn=<DivBackward0>), tensor(1.3556, grad_fn=<DivBackward0>), tensor(1.3406, grad_fn=<DivBackward0>), tensor(1.2558, grad_fn=<DivBackward0>), tensor(1.2949, grad_fn=<DivBackward0>), tensor(1.3038, grad_fn=<DivBackward0>), tensor(1.3025, grad_fn=<DivBackward0>), tensor(1.3339, grad_fn=<DivBackward0>), tensor(1.3096, grad_fn=<DivBackward0>), tensor(1.3370, grad_fn=<DivBackward0>), tensor(1.2638, grad_fn=<DivBackward0>)]\n","[1.3317312240600585, 1.1088576316833496, 1.1026176452636718, 0.9755135536193847, 0.9650239944458008, 0.9812928199768066, 0.9975232124328614, 0.9902463912963867, 0.975551986694336, 1.006764793395996, 1.0243647575378418, 1.227840042114258, 0.9422656059265136, 0.9922687530517578, 1.1161727905273438, 1.1192000389099122]\n","['117', '117', '117', '117', '117', '117', '117', '117', '117', '117', '117', '117', '117', '117', '117', '117']\n","['1.92 K', '1.92 K', '1.92 K', '1.92 K', '1.92 K', '1.92 K', '1.92 K', '1.92 K', '1.92 K', '1.92 K', '1.92 K', '1.92 K', '1.92 K', '1.92 K', '1.92 K', '1.92 K']\n"]}],"source":["import csv\n","with open('dataLab3.csv', mode='w', newline='') as file:\n","    writer = csv.writer(file)\n","    writer.writerow(['Search History', 'Recorded Accuracies', 'Recorded Losses', 'Recorded Latencies', 'Recorded Model Sizes', 'Recorded FLOPS'])\n","    for i in range(len(search_history)):\n","        writer.writerow([search_history[i], recorded_accs[i], recorded_loss[i], recorded_latencies[i], recorded_model_sizes[i], recorded_flops[i]])\n","\n","print(search_history)\n","print(recorded_accs)\n","print(recorded_loss)\n","print(recorded_latencies)\n","print(recorded_model_sizes)\n","print(recorded_flops)"]},{"cell_type":"markdown","metadata":{"id":"bxv08DrXZHFt"},"source":["# The search command in the MASE flow\n","\n","The search flow implemented in MASE is very similar to the one that you have constructed manually, the overall flow is implemented in [search.py](../machop/chop/actions/search/search.py), the following bullet points provide you pointers to the code base.\n","\n","- MaseGraph: this is the [MaseGraph](../machop/chop/passes/graph/mase_graph.py) that you have used in lab2.\n","- Search space: The base class is implemented in [base.py](../machop/chop/actions/search/search_space/base.py) , where in the same folder you can see a range of different supported search spaces.\n","- Search strategy: Similar to the search space, you can find a a base class [definition](../machop/chop/actions/search/strategies/base.py), where different strategies are also defined in the same folder.\n","- Runner: Different [runners](../machop/chop/actions/search/strategies/runners) can produce different metrics, they may also use `transforms` to help compute certain search metrics.\n","\n","This enables one to execute the search through the MASE command line interface, remember to change the name after the `--load` option.\n"]},{"cell_type":"code","execution_count":10,"metadata":{"id":"dOi0xZ8PCX8q"},"outputs":[],"source":["# !./ch search --config configs/examples/jsc_toy_by_type.toml --load your_pre_trained_ckpt\n","\n","# !./ch search --config configs/examples/jsc_toy_by_type.toml --load ../mase_output/jsc-tiny_classification_jsc_Base/software/training_ckpts/best.ckpt --load-type pl"]},{"cell_type":"markdown","metadata":{"id":"gvzdE4MSCc3g"},"source":["In this scenario, the search functionality is specified in the `toml` configuration file rather than via command-line inputs. This approach is adopted due to the multitude of configuration parameters that need to be set; encapsulating them within a single, elegant configuration file enhances reproducibility.\n","\n","In `jsc_toy_by_type.toml`, the `search_space` configuration is set in `search.search_space`, the search strategy is configured via `search.strategy`. If you are not familiar with the `toml` syntax, you can read [here](https://toml.io/en/v1.0.0)."]},{"cell_type":"markdown","metadata":{"id":"aLrs_IVDIthZ"},"source":["> In order to accomplish the following task, it is necessary to make direct modifications to the code base. This can be challenging within the Colab environment. **It is recommended to implement the task on a local setup and utilize Colab strictly as a server to execute the search command above.** Consider Colab as a dedicated server for this purpose."]},{"cell_type":"markdown","metadata":{"id":"BE4oiC-2CEFR"},"source":["With now an understanding of how the MASE flow work, consider the following tasks\n","\n","3. Implement the brute-force search as an additional search method within the system, this would be a new search strategy in MASE.\n"]},{"cell_type":"code","execution_count":11,"metadata":{},"outputs":[],"source":["# !./ch search --config configs/examples/my.toml\n","# !./ch search --config configs/examples/jsc_tiny_my_B.toml"]},{"cell_type":"markdown","metadata":{},"source":["4. Compare the brute-force search with the TPE based search, in terms of sample efficiency. Comment on the performance difference between the two search methods."]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["# !./ch search --config configs/examples/jsc_tiny_my_t.toml\n"]},{"cell_type":"code","execution_count":6,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["/bin/bash: /home/super_monkey/anaconda3/lib/libtinfo.so.6: no version information available (required by /bin/bash)\n","/bin/bash: ./ch: No such file or directory\n"]}],"source":["# ./ch search --config configs/examples/jsc_tiny_my_lab4.toml\n","# ./ch search --config configs/examples/jsc_tiny_my_lab4.toml\n","# ./ch search_my --config configs/examples/jsc_tiny_my_lab4.toml\n"]}],"metadata":{"accelerator":"GPU","colab":{"gpuType":"T4","provenance":[{"file_id":"1NINl2shLe5Uxp2IYkxmqXCJofGxBRCZs","timestamp":1705282336697}]},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.1.0"}},"nbformat":4,"nbformat_minor":0}
