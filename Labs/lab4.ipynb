{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32mINFO    \u001b[0m \u001b[34mSet logging level to info\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "\n",
    "sys.path.append('/home/super_monkey/mase_new/machop')\n",
    "import logging\n",
    "import os\n",
    "from pathlib import Path\n",
    "from pprint import pprint as pp\n",
    "\n",
    "from chop.passes.graph.utils import (\n",
    "    deepcopy_mase_graph,\n",
    "    get_mase_op,\n",
    "    get_mase_type,\n",
    "    get_node_actual_target,\n",
    "    get_parent_name,\n",
    "    get_similar_node_actual_target,\n",
    "    match_a_pattern,\n",
    "    get_node_target_by_name,\n",
    ")\n",
    "# # figure out the correct path\n",
    "# machop_path = Path(\".\").resolve().parent.parent /\"machop\"\n",
    "# assert machop_path.exists(), \"Failed to find machop at: {}\".format(machop_path)\n",
    "# sys.path.append(str(machop_path))\n",
    "\n",
    "from chop.dataset import MaseDataModule, get_dataset_info\n",
    "from chop.tools.logger import set_logging_verbosity, get_logger\n",
    "\n",
    "from chop.passes.graph.analysis import (\n",
    "    report_node_meta_param_analysis_pass,\n",
    "    profile_statistics_analysis_pass,\n",
    ")\n",
    "from chop.passes.graph import (\n",
    "    add_common_metadata_analysis_pass,\n",
    "    init_metadata_analysis_pass,\n",
    "    add_software_metadata_analysis_pass,\n",
    ")\n",
    "from chop.tools.get_input import InputGenerator\n",
    "from chop.ir.graph.mase_graph import MaseGraph\n",
    "\n",
    "from chop.models import get_model_info, get_model\n",
    "from chop.passes.graph import report_graph_analysis_pass\n",
    "from torchmetrics.classification import MulticlassAccuracy\n",
    "\n",
    "set_logging_verbosity(\"info\")\n",
    "\n",
    "logger = get_logger(\"chop\")\n",
    "logger.setLevel(logging.INFO)\n",
    "\n",
    "batch_size = 8\n",
    "model_name = \"jsc-tiny\"\n",
    "dataset_name = \"jsc\"\n",
    "\n",
    "data_module = MaseDataModule(\n",
    "    name=dataset_name,\n",
    "    batch_size=batch_size,\n",
    "    model_name=model_name,\n",
    "    num_workers=0,\n",
    ")\n",
    "data_module.prepare_data()\n",
    "data_module.setup()\n",
    "\n",
    "model_info = get_model_info(model_name)\n",
    "\n",
    "input_generator = InputGenerator(\n",
    "    data_module=data_module,\n",
    "    model_info=model_info,\n",
    "    task=\"cls\",\n",
    "    which_dataloader=\"train\",\n",
    ")\n",
    "\n",
    "dummy_in = {\"x\": next(iter(data_module.train_dataloader()))[0]}\n",
    "\n",
    "###############\n",
    "\n",
    "from torch import nn\n",
    "from chop.passes.graph.utils import get_parent_name\n",
    "\n",
    "\n",
    "# define a new model\n",
    "class JSC_Three_Linear_Layers(nn.Module):\n",
    "#     def __init__(self):\n",
    "#         super(JSC_Three_Linear_Layers, self).__init__()\n",
    "#         self.seq_blocks = nn.Sequential(\n",
    "#             nn.BatchNorm1d(16),  # 0\n",
    "#             nn.ReLU(16),  # 1\n",
    "#             nn.Linear(16, 16),  # linear  2\n",
    "#             nn.Linear(16, 16),  # linear  3\n",
    "#             nn.Linear(16, 5),  # linear  4\n",
    "#             nn.ReLU(5),  # 5\n",
    "#         )\n",
    "\n",
    "#     def forward(self, x):\n",
    "#         return self.seq_blocks(x)\n",
    "    def __init__(self):\n",
    "        super(JSC_Three_Linear_Layers, self).__init__()\n",
    "        self.seq_blocks = nn.Sequential(\n",
    "            nn.BatchNorm1d(16),\n",
    "            nn.ReLU(16),\n",
    "            nn.Linear(16, 32),  # output scaled by 2\n",
    "            nn.ReLU(32),  # scaled by 2\n",
    "            nn.Linear(32, 64),  # input scaled by 2 but output scaled by 4\n",
    "            nn.ReLU(64),  # scaled by 4\n",
    "            nn.Linear(64, 5),  # scaled by 4\n",
    "            nn.ReLU(5),\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.seq_blocks(x)\n",
    "\n",
    "model = JSC_Three_Linear_Layers()\n",
    "\n",
    "# generate the mase graph and initialize node metadata\n",
    "def reload_mg():\n",
    "    mg = MaseGraph(model=model)\n",
    "    mg, _ = init_metadata_analysis_pass(mg, None)\n",
    "    return mg\n",
    "\n",
    "\n",
    "\n",
    "###############################\n",
    "def instantiate_linear(in_features, out_features, bias):\n",
    "    if bias is not None:\n",
    "        bias = True\n",
    "    return nn.Linear(\n",
    "        in_features=in_features,\n",
    "        out_features=out_features,\n",
    "        bias=bias)\n",
    "\n",
    "\n",
    "def instantiate_relu(inplace):\n",
    "    return nn.ReLU(inplace=inplace)\n",
    "\n",
    "\n",
    "def redefine_linear_Relu_transform_pass(graph, pass_args=None):\n",
    "    main_config = pass_args.pop('config')\n",
    "    default = main_config.pop('default', None)\n",
    "    if default is None:\n",
    "        raise ValueError(f\"default value must be provided.\")\n",
    "    i = 0\n",
    "    for node in graph.fx_graph.nodes:\n",
    "        i += 1\n",
    "        # if node name is not matched, it won't be tracked\n",
    "        config = main_config.get(node.name, default)['config']\n",
    "        name = config.get(\"name\", None)\n",
    "        if name is not None:\n",
    "            ori_module = graph.modules[node.target]\n",
    "            if name == \"inplace\":\n",
    "                inplace = ori_module.inplace\n",
    "                inplace = inplace * config[\"channel_multiplier\"]\n",
    "                new_module = instantiate_relu(inplace)\n",
    "                parent_name, name = get_parent_name(node.target)\n",
    "                setattr(graph.modules[parent_name], name, new_module)\n",
    "                pass\n",
    "            else:\n",
    "                in_features = ori_module.in_features\n",
    "                out_features = ori_module.out_features\n",
    "                bias = ori_module.bias\n",
    "                if name == \"output_only\":\n",
    "                    out_features = out_features * config[\"channel_multiplier\"]\n",
    "                elif name == \"both\":\n",
    "                    if type(config[\"channel_multiplier\"])== int:\n",
    "                        in_features = in_features * config[\"channel_multiplier\"]\n",
    "                        out_features = out_features * config[\"channel_multiplier\"]\n",
    "                    else:\n",
    "                        in_features = in_features * config[\"channel_multiplier\"][0]\n",
    "                        out_features = out_features * config[\"channel_multiplier\"][1]\n",
    "                elif name == \"input_only\":\n",
    "                    in_features = in_features * config[\"channel_multiplier\"]\n",
    "                new_module = instantiate_linear(in_features, out_features, bias)\n",
    "                parent_name, name = get_parent_name(node.target)\n",
    "                setattr(graph.modules[parent_name], name, new_module)\n",
    "    return graph, {}\n",
    "mg=reload_mg()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "pass_config = {\n",
    "    \"by\": \"name\",\n",
    "    \"default\": {\"config\": {\"name\": None}},\n",
    "    \"seq_blocks_1\": {\n",
    "        \"config\": {\n",
    "            \"name\": \"inplace\",\n",
    "            \"channel_multiplier\": 2,\n",
    "        }\n",
    "    },\n",
    "    \"seq_blocks_2\": {\n",
    "        \"config\": {\n",
    "            \"name\": \"output_only\",\n",
    "            \"channel_multiplier\": 2,\n",
    "        }\n",
    "    },\n",
    "    \"seq_blocks_4\": {\n",
    "        \"config\": {\n",
    "            \"name\": \"both\",\n",
    "            \"channel_multiplier\": 2,\n",
    "        }\n",
    "    },\n",
    "    \"seq_blocks_6\": {\n",
    "        \"config\": {\n",
    "            \"name\": \"input_only\",\n",
    "            \"channel_multiplier\": 2,\n",
    "        }\n",
    "    },\n",
    "    \"seq_blocks_7\": {\n",
    "        \"config\": {\n",
    "            \"name\": \"inplace\",\n",
    "            \"channel_multiplier\": 2,\n",
    "        }\n",
    "    },\n",
    "}\n",
    "# this performs the architecture transformation based on the config\n",
    "mg, _ = redefine_linear_Relu_transform_pass(\n",
    "    graph=mg, pass_args={\"config\": pass_config})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "pass_base_cfg = {\n",
    "    \"by\": \"name\",\n",
    "    \"default\": {\"config\": {\"name\": None}},\n",
    "    \"seq_blocks_1\": {\n",
    "        \"config\": {\n",
    "            \"name\": \"inplace\",\n",
    "            \"channel_multiplier\": 1,\n",
    "        }\n",
    "    },\n",
    "    \"seq_blocks_2\": {\n",
    "        \"config\": {\n",
    "            \"name\": \"both\",\n",
    "            \"channel_multiplier\": 1,\n",
    "        }\n",
    "    },\n",
    "    \"seq_blocks_4\": {\n",
    "        \"config\": {\n",
    "            \"name\": \"both\",\n",
    "            \"channel_multiplier\": 1,\n",
    "        }\n",
    "    },\n",
    "    \"seq_blocks_6\": {\n",
    "        \"config\": {\n",
    "            \"name\": \"both\",\n",
    "            \"channel_multiplier\": 1,\n",
    "        }\n",
    "    },\n",
    "    \"seq_blocks_7\": {\n",
    "        \"config\": {\n",
    "            \"name\": \"inplace\",\n",
    "            \"channel_multiplier\": 1,\n",
    "        }\n",
    "    },\n",
    "}\n",
    "import copy\n",
    "# build a search space\n",
    "seq=[1,2,4,6,7]\n",
    "mul=[1,2,4,8,16]\n",
    "\n",
    "search_spaces = []\n",
    "search_history=[]\n",
    "for x in (mul):\n",
    "    for y in (mul):\n",
    "        C_M = [1,(1,x),(x,y),(y,1),1]\n",
    "        for j, cm in zip(seq,C_M):\n",
    "            pass_base_cfg[f'seq_blocks_{j}']['config']['channel_multiplier'] = cm\n",
    "            # dict.copy() and dict(dict) only perform shallow copies\n",
    "            # in fact, only primitive data types in python are doing implicit copy when a = b happens\n",
    "        search_spaces.append(copy.deepcopy(pass_base_cfg))\n",
    "        search_history.append(C_M)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "pass_base_cfg = {\n",
    "    \"by\": \"name\",\n",
    "    \"default\": {\"config\": {\"name\": None}},\n",
    "    \"seq_blocks_2\": {\n",
    "        \"config\": {\n",
    "            \"name\": \"output_only\",\n",
    "            \"channel_multiplier\": 1,\n",
    "        }\n",
    "    },\n",
    "    \"seq_blocks_4\": {\n",
    "        \"config\": {\n",
    "            \"name\": \"both\",\n",
    "            \"channel_multiplier\": 1,\n",
    "        }\n",
    "    },\n",
    "    \"seq_blocks_6\": {\n",
    "        \"config\": {\n",
    "            \"name\": \"input_only\",\n",
    "            \"channel_multiplier\": 1,\n",
    "        }\n",
    "    },\n",
    "}\n",
    "import copy\n",
    "# build a search space\n",
    "seq=[2,4,6]\n",
    "mul=[1,2,4,8,16,64]\n",
    "\n",
    "search_spaces = []\n",
    "search_history=[]\n",
    "for x in (mul):\n",
    "    C_M = [x]\n",
    "    for cm in C_M:\n",
    "        pass_base_cfg[f'seq_blocks_2']['config']['channel_multiplier'] = cm\n",
    "        pass_base_cfg[f'seq_blocks_4']['config']['channel_multiplier'] = cm\n",
    "        pass_base_cfg[f'seq_blocks_6']['config']['channel_multiplier'] = cm\n",
    "        # dict.copy() and dict(dict) only perform shallow copies\n",
    "        # in fact, only primitive data types in python are doing implicit copy when a = b happens\n",
    "        search_spaces.append(copy.deepcopy(pass_base_cfg))\n",
    "        search_history.append(C_M)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch \n",
    "import numpy as np\n",
    "from deepspeed.profiling.flops_profiler import get_model_profile\n",
    "\n",
    "from chop.passes.graph.transforms import (\n",
    "    quantize_transform_pass,\n",
    "    summarize_quantization_analysis_pass,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "[2024-02-11 14:56:51,411] [INFO] [profiler.py:1205:get_model_profile] Flops profiler warming-up...\n",
      "[2024-02-11 14:56:51,412] [INFO] [profiler.py:80:start_profile] Flops profiler started\n",
      "\n",
      "-------------------------- DeepSpeed Flops Profiler --------------------------\n",
      "Profile Summary at step 1:\n",
      "Notations:\n",
      "data parallel size (dp_size), model parallel size(mp_size),\n",
      "number of parameters (params), number of multiply-accumulate operations(MACs),\n",
      "number of floating-point operations (flops), floating-point operations per second (FLOPS),\n",
      "fwd latency (forward propagation latency), bwd latency (backward propagation latency),\n",
      "step (weights update latency), iter latency (sum of fwd, bwd and step latency)\n",
      "\n",
      "params per GPU:                                                         3.01 K  \n",
      "params of model = params per GPU * mp_size:                             0       \n",
      "fwd MACs per GPU:                                                       23.04 KMACs\n",
      "fwd flops per GPU:                                                      47.27 K \n",
      "fwd flops of model = fwd flops per GPU * mp_size:                       47.27 K \n",
      "fwd latency:                                                            622.51 us\n",
      "fwd FLOPS per GPU = fwd flops per GPU / fwd latency:                    75.94 MFLOPS\n",
      "\n",
      "----------------------------- Aggregated Profile per GPU -----------------------------\n",
      "Top 1 modules in terms of params, MACs or fwd latency at different model depths:\n",
      "depth 0:\n",
      "    params      - {'GraphModule': '3.01 K'}\n",
      "    MACs        - {'GraphModule': '23.04 KMACs'}\n",
      "    fwd latency - {'GraphModule': '622.51 us'}\n",
      "depth 1:\n",
      "    params      - {'Module': '3.01 K'}\n",
      "    MACs        - {'Module': '23.04 KMACs'}\n",
      "    fwd latency - {'Module': '455.86 us'}\n",
      "\n",
      "------------------------------ Detailed Profile per GPU ------------------------------\n",
      "Each module profile is listed after its name in the following order: \n",
      "params, percentage of total params, MACs, percentage of total MACs, fwd latency, percentage of total fwd latency, fwd FLOPS\n",
      "\n",
      "Note: 1. A module can have torch.nn.module or torch.nn.functional to compute logits (e.g. CrossEntropyLoss). They are not counted as submodules, thus not to be printed out. However they make up the difference between a parent's MACs (or latency) and the sum of its submodules'.\n",
      "2. Number of floating-point operations is a theoretical estimation, thus FLOPS computed using that could be larger than the maximum system throughput.\n",
      "3. The fwd latency listed in the top module's profile is directly captured at the module forward function in PyTorch, thus it's less than the fwd latency shown above which is captured in DeepSpeed.\n",
      "\n",
      "GraphModule(\n",
      "  3.01 K = 100% Params, 23.04 KMACs = 100% MACs, 622.51 us = 100% latency, 75.94 MFLOPS\n",
      "  (seq_blocks): Module(\n",
      "    3.01 K = 100% Params, 23.04 KMACs = 100% MACs, 455.86 us = 73.23% latency, 103.7 MFLOPS\n",
      "    (0): BatchNorm1d(32 = 1.06% Params, 0 MACs = 0% MACs, 129.46 us = 20.8% latency, 1.98 MFLOPS, 16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (1): ReLU(0 = 0% Params, 0 MACs = 0% MACs, 60.56 us = 9.73% latency, 2.11 MFLOPS, inplace=True)\n",
      "    (2): Linear(544 = 18.06% Params, 4.1 KMACs = 17.78% MACs, 63.9 us = 10.26% latency, 128.21 MFLOPS, in_features=16, out_features=32, bias=True)\n",
      "    (3): ReLU(0 = 0% Params, 0 MACs = 0% MACs, 52.93 us = 8.5% latency, 4.84 MFLOPS, inplace=True)\n",
      "    (4): Linear(2.11 K = 70.1% Params, 16.38 KMACs = 71.11% MACs, 46.25 us = 7.43% latency, 708.45 MFLOPS, in_features=32, out_features=64, bias=True)\n",
      "    (5): ReLU(0 = 0% Params, 0 MACs = 0% MACs, 29.8 us = 4.79% latency, 17.18 MFLOPS, inplace=True)\n",
      "    (6): Linear(325 = 10.79% Params, 2.56 KMACs = 11.11% MACs, 43.15 us = 6.93% latency, 118.65 MFLOPS, in_features=64, out_features=5, bias=True)\n",
      "    (7): ReLU(0 = 0% Params, 0 MACs = 0% MACs, 29.8 us = 4.79% latency, 1.34 MFLOPS, inplace=True)\n",
      "  )\n",
      ")\n",
      "\n",
      "\n",
      "\n",
      "def forward(self, x):\n",
      "    seq_blocks_0 = getattr(self.seq_blocks, \"0\")(x);  x = None\n",
      "    seq_blocks_1 = getattr(self.seq_blocks, \"1\")(seq_blocks_0);  seq_blocks_0 = None\n",
      "    seq_blocks_2 = getattr(self.seq_blocks, \"2\")(seq_blocks_1);  seq_blocks_1 = None\n",
      "    seq_blocks_3 = getattr(self.seq_blocks, \"3\")(seq_blocks_2);  seq_blocks_2 = None\n",
      "    seq_blocks_4 = getattr(self.seq_blocks, \"4\")(seq_blocks_3);  seq_blocks_3 = None\n",
      "    seq_blocks_5 = getattr(self.seq_blocks, \"5\")(seq_blocks_4);  seq_blocks_4 = None\n",
      "    seq_blocks_6 = getattr(self.seq_blocks, \"6\")(seq_blocks_5);  seq_blocks_5 = None\n",
      "    seq_blocks_7 = getattr(self.seq_blocks, \"7\")(seq_blocks_6);  seq_blocks_6 = None\n",
      "    return seq_blocks_7\n",
      "    \n",
      "# To see more debug info, please use `graph_module.print_readable()`\n",
      "------------------------------------------------------------------------------\n",
      "[2024-02-11 14:56:51,416] [INFO] [profiler.py:226:end_profile] Flops profiler finished\n",
      "1\n",
      "[2024-02-11 14:56:51,466] [INFO] [profiler.py:1205:get_model_profile] Flops profiler warming-up...\n",
      "[2024-02-11 14:56:51,467] [INFO] [profiler.py:80:start_profile] Flops profiler started\n",
      "\n",
      "-------------------------- DeepSpeed Flops Profiler --------------------------\n",
      "Profile Summary at step 1:\n",
      "Notations:\n",
      "data parallel size (dp_size), model parallel size(mp_size),\n",
      "number of parameters (params), number of multiply-accumulate operations(MACs),\n",
      "number of floating-point operations (flops), floating-point operations per second (FLOPS),\n",
      "fwd latency (forward propagation latency), bwd latency (backward propagation latency),\n",
      "step (weights update latency), iter latency (sum of fwd, bwd and step latency)\n",
      "\n",
      "params per GPU:                                                         10.09 K \n",
      "params of model = params per GPU * mp_size:                             0       \n",
      "fwd MACs per GPU:                                                       78.85 KMACs\n",
      "fwd flops per GPU:                                                      159.66 K\n",
      "fwd flops of model = fwd flops per GPU * mp_size:                       159.66 K\n",
      "fwd latency:                                                            665.9 us\n",
      "fwd FLOPS per GPU = fwd flops per GPU / fwd latency:                    239.76 MFLOPS\n",
      "\n",
      "----------------------------- Aggregated Profile per GPU -----------------------------\n",
      "Top 1 modules in terms of params, MACs or fwd latency at different model depths:\n",
      "depth 0:\n",
      "    params      - {'GraphModule': '10.09 K'}\n",
      "    MACs        - {'GraphModule': '78.85 KMACs'}\n",
      "    fwd latency - {'GraphModule': '665.9 us'}\n",
      "depth 1:\n",
      "    params      - {'Module': '10.09 K'}\n",
      "    MACs        - {'Module': '78.85 KMACs'}\n",
      "    fwd latency - {'Module': '475.65 us'}\n",
      "\n",
      "------------------------------ Detailed Profile per GPU ------------------------------\n",
      "Each module profile is listed after its name in the following order: \n",
      "params, percentage of total params, MACs, percentage of total MACs, fwd latency, percentage of total fwd latency, fwd FLOPS\n",
      "\n",
      "Note: 1. A module can have torch.nn.module or torch.nn.functional to compute logits (e.g. CrossEntropyLoss). They are not counted as submodules, thus not to be printed out. However they make up the difference between a parent's MACs (or latency) and the sum of its submodules'.\n",
      "2. Number of floating-point operations is a theoretical estimation, thus FLOPS computed using that could be larger than the maximum system throughput.\n",
      "3. The fwd latency listed in the top module's profile is directly captured at the module forward function in PyTorch, thus it's less than the fwd latency shown above which is captured in DeepSpeed.\n",
      "\n",
      "GraphModule(\n",
      "  10.09 K = 100% Params, 78.85 KMACs = 100% MACs, 665.9 us = 100% latency, 239.76 MFLOPS\n",
      "  (seq_blocks): Module(\n",
      "    10.09 K = 100% Params, 78.85 KMACs = 100% MACs, 475.65 us = 71.43% latency, 335.66 MFLOPS\n",
      "    (0): BatchNorm1d(32 = 0.32% Params, 0 MACs = 0% MACs, 160.93 us = 24.17% latency, 1.59 MFLOPS, 16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (1): ReLU(0 = 0% Params, 0 MACs = 0% MACs, 46.25 us = 6.95% latency, 2.77 MFLOPS, inplace=True)\n",
      "    (2): Linear(1.09 K = 10.79% Params, 8.19 KMACs = 10.39% MACs, 66.52 us = 9.99% latency, 246.31 MFLOPS, in_features=16, out_features=64, bias=True)\n",
      "    (3): ReLU(0 = 0% Params, 0 MACs = 0% MACs, 35.29 us = 5.3% latency, 14.51 MFLOPS, inplace=True)\n",
      "    (4): Linear(8.32 K = 82.5% Params, 65.54 KMACs = 83.12% MACs, 48.64 us = 7.3% latency, 2.69 GFLOPS, in_features=64, out_features=128, bias=True)\n",
      "    (5): ReLU(0 = 0% Params, 0 MACs = 0% MACs, 30.76 us = 4.62% latency, 33.29 MFLOPS, inplace=True)\n",
      "    (6): Linear(645 = 6.4% Params, 5.12 KMACs = 6.49% MACs, 59.13 us = 8.88% latency, 173.18 MFLOPS, in_features=128, out_features=5, bias=True)\n",
      "    (7): ReLU(0 = 0% Params, 0 MACs = 0% MACs, 28.13 us = 4.22% latency, 1.42 MFLOPS, inplace=True)\n",
      "  )\n",
      ")\n",
      "\n",
      "\n",
      "\n",
      "def forward(self, x):\n",
      "    seq_blocks_0 = getattr(self.seq_blocks, \"0\")(x);  x = None\n",
      "    seq_blocks_1 = getattr(self.seq_blocks, \"1\")(seq_blocks_0);  seq_blocks_0 = None\n",
      "    seq_blocks_2 = getattr(self.seq_blocks, \"2\")(seq_blocks_1);  seq_blocks_1 = None\n",
      "    seq_blocks_3 = getattr(self.seq_blocks, \"3\")(seq_blocks_2);  seq_blocks_2 = None\n",
      "    seq_blocks_4 = getattr(self.seq_blocks, \"4\")(seq_blocks_3);  seq_blocks_3 = None\n",
      "    seq_blocks_5 = getattr(self.seq_blocks, \"5\")(seq_blocks_4);  seq_blocks_4 = None\n",
      "    seq_blocks_6 = getattr(self.seq_blocks, \"6\")(seq_blocks_5);  seq_blocks_5 = None\n",
      "    seq_blocks_7 = getattr(self.seq_blocks, \"7\")(seq_blocks_6);  seq_blocks_6 = None\n",
      "    return seq_blocks_7\n",
      "    \n",
      "# To see more debug info, please use `graph_module.print_readable()`\n",
      "------------------------------------------------------------------------------\n",
      "[2024-02-11 14:56:51,470] [INFO] [profiler.py:226:end_profile] Flops profiler finished\n",
      "2\n",
      "[2024-02-11 14:56:51,509] [INFO] [profiler.py:1205:get_model_profile] Flops profiler warming-up...\n",
      "[2024-02-11 14:56:51,509] [INFO] [profiler.py:80:start_profile] Flops profiler started\n",
      "\n",
      "-------------------------- DeepSpeed Flops Profiler --------------------------\n",
      "Profile Summary at step 1:\n",
      "Notations:\n",
      "data parallel size (dp_size), model parallel size(mp_size),\n",
      "number of parameters (params), number of multiply-accumulate operations(MACs),\n",
      "number of floating-point operations (flops), floating-point operations per second (FLOPS),\n",
      "fwd latency (forward propagation latency), bwd latency (backward propagation latency),\n",
      "step (weights update latency), iter latency (sum of fwd, bwd and step latency)\n",
      "\n",
      "params per GPU:                                                         36.52 K \n",
      "params of model = params per GPU * mp_size:                             0       \n",
      "fwd MACs per GPU:                                                       288.77 KMACs\n",
      "fwd flops per GPU:                                                      581.03 K\n",
      "fwd flops of model = fwd flops per GPU * mp_size:                       581.03 K\n",
      "fwd latency:                                                            656.13 us\n",
      "fwd FLOPS per GPU = fwd flops per GPU / fwd latency:                    885.55 MFLOPS\n",
      "\n",
      "----------------------------- Aggregated Profile per GPU -----------------------------\n",
      "Top 1 modules in terms of params, MACs or fwd latency at different model depths:\n",
      "depth 0:\n",
      "    params      - {'GraphModule': '36.52 K'}\n",
      "    MACs        - {'GraphModule': '288.77 KMACs'}\n",
      "    fwd latency - {'GraphModule': '656.13 us'}\n",
      "depth 1:\n",
      "    params      - {'Module': '36.52 K'}\n",
      "    MACs        - {'Module': '288.77 KMACs'}\n",
      "    fwd latency - {'Module': '491.86 us'}\n",
      "\n",
      "------------------------------ Detailed Profile per GPU ------------------------------\n",
      "Each module profile is listed after its name in the following order: \n",
      "params, percentage of total params, MACs, percentage of total MACs, fwd latency, percentage of total fwd latency, fwd FLOPS\n",
      "\n",
      "Note: 1. A module can have torch.nn.module or torch.nn.functional to compute logits (e.g. CrossEntropyLoss). They are not counted as submodules, thus not to be printed out. However they make up the difference between a parent's MACs (or latency) and the sum of its submodules'.\n",
      "2. Number of floating-point operations is a theoretical estimation, thus FLOPS computed using that could be larger than the maximum system throughput.\n",
      "3. The fwd latency listed in the top module's profile is directly captured at the module forward function in PyTorch, thus it's less than the fwd latency shown above which is captured in DeepSpeed.\n",
      "\n",
      "GraphModule(\n",
      "  36.52 K = 100% Params, 288.77 KMACs = 100% MACs, 656.13 us = 100% latency, 885.55 MFLOPS\n",
      "  (seq_blocks): Module(\n",
      "    36.52 K = 100% Params, 288.77 KMACs = 100% MACs, 491.86 us = 74.96% latency, 1.18 GFLOPS\n",
      "    (0): BatchNorm1d(32 = 0.09% Params, 0 MACs = 0% MACs, 201.46 us = 30.7% latency, 1.27 MFLOPS, 16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (1): ReLU(0 = 0% Params, 0 MACs = 0% MACs, 55.07 us = 8.39% latency, 2.32 MFLOPS, inplace=True)\n",
      "    (2): Linear(2.18 K = 5.96% Params, 16.38 KMACs = 5.67% MACs, 58.89 us = 8.98% latency, 556.43 MFLOPS, in_features=16, out_features=128, bias=True)\n",
      "    (3): ReLU(0 = 0% Params, 0 MACs = 0% MACs, 30.04 us = 4.58% latency, 34.09 MFLOPS, inplace=True)\n",
      "    (4): Linear(33.02 K = 90.43% Params, 262.14 KMACs = 90.78% MACs, 45.54 us = 6.94% latency, 11.51 GFLOPS, in_features=128, out_features=256, bias=True)\n",
      "    (5): ReLU(0 = 0% Params, 0 MACs = 0% MACs, 29.56 us = 4.51% latency, 69.27 MFLOPS, inplace=True)\n",
      "    (6): Linear(1.28 K = 3.52% Params, 10.24 KMACs = 3.55% MACs, 42.92 us = 6.54% latency, 477.22 MFLOPS, in_features=256, out_features=5, bias=True)\n",
      "    (7): ReLU(0 = 0% Params, 0 MACs = 0% MACs, 28.37 us = 4.32% latency, 1.41 MFLOPS, inplace=True)\n",
      "  )\n",
      ")\n",
      "\n",
      "\n",
      "\n",
      "def forward(self, x):\n",
      "    seq_blocks_0 = getattr(self.seq_blocks, \"0\")(x);  x = None\n",
      "    seq_blocks_1 = getattr(self.seq_blocks, \"1\")(seq_blocks_0);  seq_blocks_0 = None\n",
      "    seq_blocks_2 = getattr(self.seq_blocks, \"2\")(seq_blocks_1);  seq_blocks_1 = None\n",
      "    seq_blocks_3 = getattr(self.seq_blocks, \"3\")(seq_blocks_2);  seq_blocks_2 = None\n",
      "    seq_blocks_4 = getattr(self.seq_blocks, \"4\")(seq_blocks_3);  seq_blocks_3 = None\n",
      "    seq_blocks_5 = getattr(self.seq_blocks, \"5\")(seq_blocks_4);  seq_blocks_4 = None\n",
      "    seq_blocks_6 = getattr(self.seq_blocks, \"6\")(seq_blocks_5);  seq_blocks_5 = None\n",
      "    seq_blocks_7 = getattr(self.seq_blocks, \"7\")(seq_blocks_6);  seq_blocks_6 = None\n",
      "    return seq_blocks_7\n",
      "    \n",
      "# To see more debug info, please use `graph_module.print_readable()`\n",
      "------------------------------------------------------------------------------\n",
      "[2024-02-11 14:56:51,512] [INFO] [profiler.py:226:end_profile] Flops profiler finished\n",
      "3\n",
      "[2024-02-11 14:56:51,554] [INFO] [profiler.py:1205:get_model_profile] Flops profiler warming-up...\n",
      "[2024-02-11 14:56:51,555] [INFO] [profiler.py:80:start_profile] Flops profiler started\n",
      "\n",
      "-------------------------- DeepSpeed Flops Profiler --------------------------\n",
      "Profile Summary at step 1:\n",
      "Notations:\n",
      "data parallel size (dp_size), model parallel size(mp_size),\n",
      "number of parameters (params), number of multiply-accumulate operations(MACs),\n",
      "number of floating-point operations (flops), floating-point operations per second (FLOPS),\n",
      "fwd latency (forward propagation latency), bwd latency (backward propagation latency),\n",
      "step (weights update latency), iter latency (sum of fwd, bwd and step latency)\n",
      "\n",
      "params per GPU:                                                         138.53 K\n",
      "params of model = params per GPU * mp_size:                             0       \n",
      "fwd MACs per GPU:                                                       1.1 MMACs\n",
      "fwd flops per GPU:                                                      2.21 M  \n",
      "fwd flops of model = fwd flops per GPU * mp_size:                       2.21 M  \n",
      "fwd latency:                                                            541.21 us\n",
      "fwd FLOPS per GPU = fwd flops per GPU / fwd latency:                    4.08 GFLOPS\n",
      "\n",
      "----------------------------- Aggregated Profile per GPU -----------------------------\n",
      "Top 1 modules in terms of params, MACs or fwd latency at different model depths:\n",
      "depth 0:\n",
      "    params      - {'GraphModule': '138.53 K'}\n",
      "    MACs        - {'GraphModule': '1.1 MMACs'}\n",
      "    fwd latency - {'GraphModule': '541.21 us'}\n",
      "depth 1:\n",
      "    params      - {'Module': '138.53 K'}\n",
      "    MACs        - {'Module': '1.1 MMACs'}\n",
      "    fwd latency - {'Module': '378.85 us'}\n",
      "\n",
      "------------------------------ Detailed Profile per GPU ------------------------------\n",
      "Each module profile is listed after its name in the following order: \n",
      "params, percentage of total params, MACs, percentage of total MACs, fwd latency, percentage of total fwd latency, fwd FLOPS\n",
      "\n",
      "Note: 1. A module can have torch.nn.module or torch.nn.functional to compute logits (e.g. CrossEntropyLoss). They are not counted as submodules, thus not to be printed out. However they make up the difference between a parent's MACs (or latency) and the sum of its submodules'.\n",
      "2. Number of floating-point operations is a theoretical estimation, thus FLOPS computed using that could be larger than the maximum system throughput.\n",
      "3. The fwd latency listed in the top module's profile is directly captured at the module forward function in PyTorch, thus it's less than the fwd latency shown above which is captured in DeepSpeed.\n",
      "\n",
      "GraphModule(\n",
      "  138.53 K = 100% Params, 1.1 MMACs = 100% MACs, 541.21 us = 100% latency, 4.08 GFLOPS\n",
      "  (seq_blocks): Module(\n",
      "    138.53 K = 100% Params, 1.1 MMACs = 100% MACs, 378.85 us = 70% latency, 5.83 GFLOPS\n",
      "    (0): BatchNorm1d(32 = 0.02% Params, 0 MACs = 0% MACs, 67.47 us = 12.47% latency, 3.79 MFLOPS, 16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (1): ReLU(0 = 0% Params, 0 MACs = 0% MACs, 36.72 us = 6.78% latency, 3.49 MFLOPS, inplace=True)\n",
      "    (2): Linear(4.35 K = 3.14% Params, 32.77 KMACs = 2.97% MACs, 76.06 us = 14.05% latency, 861.69 MFLOPS, in_features=16, out_features=256, bias=True)\n",
      "    (3): ReLU(0 = 0% Params, 0 MACs = 0% MACs, 31.23 us = 5.77% latency, 65.57 MFLOPS, inplace=True)\n",
      "    (4): Linear(131.58 K = 94.98% Params, 1.05 MMACs = 95.17% MACs, 59.84 us = 11.06% latency, 35.04 GFLOPS, in_features=256, out_features=512, bias=True)\n",
      "    (5): ReLU(0 = 0% Params, 0 MACs = 0% MACs, 29.8 us = 5.51% latency, 137.44 MFLOPS, inplace=True)\n",
      "    (6): Linear(2.56 K = 1.85% Params, 20.48 KMACs = 1.86% MACs, 49.11 us = 9.07% latency, 833.97 MFLOPS, in_features=512, out_features=5, bias=True)\n",
      "    (7): ReLU(0 = 0% Params, 0 MACs = 0% MACs, 28.61 us = 5.29% latency, 1.4 MFLOPS, inplace=True)\n",
      "  )\n",
      ")\n",
      "\n",
      "\n",
      "\n",
      "def forward(self, x):\n",
      "    seq_blocks_0 = getattr(self.seq_blocks, \"0\")(x);  x = None\n",
      "    seq_blocks_1 = getattr(self.seq_blocks, \"1\")(seq_blocks_0);  seq_blocks_0 = None\n",
      "    seq_blocks_2 = getattr(self.seq_blocks, \"2\")(seq_blocks_1);  seq_blocks_1 = None\n",
      "    seq_blocks_3 = getattr(self.seq_blocks, \"3\")(seq_blocks_2);  seq_blocks_2 = None\n",
      "    seq_blocks_4 = getattr(self.seq_blocks, \"4\")(seq_blocks_3);  seq_blocks_3 = None\n",
      "    seq_blocks_5 = getattr(self.seq_blocks, \"5\")(seq_blocks_4);  seq_blocks_4 = None\n",
      "    seq_blocks_6 = getattr(self.seq_blocks, \"6\")(seq_blocks_5);  seq_blocks_5 = None\n",
      "    seq_blocks_7 = getattr(self.seq_blocks, \"7\")(seq_blocks_6);  seq_blocks_6 = None\n",
      "    return seq_blocks_7\n",
      "    \n",
      "# To see more debug info, please use `graph_module.print_readable()`\n",
      "------------------------------------------------------------------------------\n",
      "[2024-02-11 14:56:51,558] [INFO] [profiler.py:226:end_profile] Flops profiler finished\n",
      "4\n",
      "[2024-02-11 14:56:51,603] [INFO] [profiler.py:1205:get_model_profile] Flops profiler warming-up...\n",
      "[2024-02-11 14:56:51,604] [INFO] [profiler.py:80:start_profile] Flops profiler started\n",
      "\n",
      "-------------------------- DeepSpeed Flops Profiler --------------------------\n",
      "Profile Summary at step 1:\n",
      "Notations:\n",
      "data parallel size (dp_size), model parallel size(mp_size),\n",
      "number of parameters (params), number of multiply-accumulate operations(MACs),\n",
      "number of floating-point operations (flops), floating-point operations per second (FLOPS),\n",
      "fwd latency (forward propagation latency), bwd latency (backward propagation latency),\n",
      "step (weights update latency), iter latency (sum of fwd, bwd and step latency)\n",
      "\n",
      "params per GPU:                                                         539.17 K\n",
      "params of model = params per GPU * mp_size:                             0       \n",
      "fwd MACs per GPU:                                                       4.3 MMACs\n",
      "fwd flops per GPU:                                                      8.61 M  \n",
      "fwd flops of model = fwd flops per GPU * mp_size:                       8.61 M  \n",
      "fwd latency:                                                            746.25 us\n",
      "fwd FLOPS per GPU = fwd flops per GPU / fwd latency:                    11.54 GFLOPS\n",
      "\n",
      "----------------------------- Aggregated Profile per GPU -----------------------------\n",
      "Top 1 modules in terms of params, MACs or fwd latency at different model depths:\n",
      "depth 0:\n",
      "    params      - {'GraphModule': '539.17 K'}\n",
      "    MACs        - {'GraphModule': '4.3 MMACs'}\n",
      "    fwd latency - {'GraphModule': '746.25 us'}\n",
      "depth 1:\n",
      "    params      - {'Module': '539.17 K'}\n",
      "    MACs        - {'Module': '4.3 MMACs'}\n",
      "    fwd latency - {'Module': '572.44 us'}\n",
      "\n",
      "------------------------------ Detailed Profile per GPU ------------------------------\n",
      "Each module profile is listed after its name in the following order: \n",
      "params, percentage of total params, MACs, percentage of total MACs, fwd latency, percentage of total fwd latency, fwd FLOPS\n",
      "\n",
      "Note: 1. A module can have torch.nn.module or torch.nn.functional to compute logits (e.g. CrossEntropyLoss). They are not counted as submodules, thus not to be printed out. However they make up the difference between a parent's MACs (or latency) and the sum of its submodules'.\n",
      "2. Number of floating-point operations is a theoretical estimation, thus FLOPS computed using that could be larger than the maximum system throughput.\n",
      "3. The fwd latency listed in the top module's profile is directly captured at the module forward function in PyTorch, thus it's less than the fwd latency shown above which is captured in DeepSpeed.\n",
      "\n",
      "GraphModule(\n",
      "  539.17 K = 100% Params, 4.3 MMACs = 100% MACs, 746.25 us = 100% latency, 11.54 GFLOPS\n",
      "  (seq_blocks): Module(\n",
      "    539.17 K = 100% Params, 4.3 MMACs = 100% MACs, 572.44 us = 76.71% latency, 15.05 GFLOPS\n",
      "    (0): BatchNorm1d(32 = 0.01% Params, 0 MACs = 0% MACs, 193.36 us = 25.91% latency, 1.32 MFLOPS, 16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (1): ReLU(0 = 0% Params, 0 MACs = 0% MACs, 34.81 us = 4.66% latency, 3.68 MFLOPS, inplace=True)\n",
      "    (2): Linear(8.7 K = 1.61% Params, 65.54 KMACs = 1.52% MACs, 106.1 us = 14.22% latency, 1.24 GFLOPS, in_features=16, out_features=512, bias=True)\n",
      "    (3): ReLU(0 = 0% Params, 0 MACs = 0% MACs, 29.8 us = 3.99% latency, 137.44 MFLOPS, inplace=True)\n",
      "    (4): Linear(525.31 K = 97.43% Params, 4.19 MMACs = 97.52% MACs, 96.08 us = 12.88% latency, 87.31 GFLOPS, in_features=512, out_features=1024, bias=True)\n",
      "    (5): ReLU(0 = 0% Params, 0 MACs = 0% MACs, 30.04 us = 4.03% latency, 272.7 MFLOPS, inplace=True)\n",
      "    (6): Linear(5.12 K = 0.95% Params, 40.96 KMACs = 0.95% MACs, 54.6 us = 7.32% latency, 1.5 GFLOPS, in_features=1024, out_features=5, bias=True)\n",
      "    (7): ReLU(0 = 0% Params, 0 MACs = 0% MACs, 27.66 us = 3.71% latency, 1.45 MFLOPS, inplace=True)\n",
      "  )\n",
      ")\n",
      "\n",
      "\n",
      "\n",
      "def forward(self, x):\n",
      "    seq_blocks_0 = getattr(self.seq_blocks, \"0\")(x);  x = None\n",
      "    seq_blocks_1 = getattr(self.seq_blocks, \"1\")(seq_blocks_0);  seq_blocks_0 = None\n",
      "    seq_blocks_2 = getattr(self.seq_blocks, \"2\")(seq_blocks_1);  seq_blocks_1 = None\n",
      "    seq_blocks_3 = getattr(self.seq_blocks, \"3\")(seq_blocks_2);  seq_blocks_2 = None\n",
      "    seq_blocks_4 = getattr(self.seq_blocks, \"4\")(seq_blocks_3);  seq_blocks_3 = None\n",
      "    seq_blocks_5 = getattr(self.seq_blocks, \"5\")(seq_blocks_4);  seq_blocks_4 = None\n",
      "    seq_blocks_6 = getattr(self.seq_blocks, \"6\")(seq_blocks_5);  seq_blocks_5 = None\n",
      "    seq_blocks_7 = getattr(self.seq_blocks, \"7\")(seq_blocks_6);  seq_blocks_6 = None\n",
      "    return seq_blocks_7\n",
      "    \n",
      "# To see more debug info, please use `graph_module.print_readable()`\n",
      "------------------------------------------------------------------------------\n",
      "[2024-02-11 14:56:51,607] [INFO] [profiler.py:226:end_profile] Flops profiler finished\n",
      "5\n",
      "[2024-02-11 14:56:51,685] [INFO] [profiler.py:1205:get_model_profile] Flops profiler warming-up...\n",
      "[2024-02-11 14:56:51,689] [INFO] [profiler.py:80:start_profile] Flops profiler started\n",
      "\n",
      "-------------------------- DeepSpeed Flops Profiler --------------------------\n",
      "Profile Summary at step 1:\n",
      "Notations:\n",
      "data parallel size (dp_size), model parallel size(mp_size),\n",
      "number of parameters (params), number of multiply-accumulate operations(MACs),\n",
      "number of floating-point operations (flops), floating-point operations per second (FLOPS),\n",
      "fwd latency (forward propagation latency), bwd latency (backward propagation latency),\n",
      "step (weights update latency), iter latency (sum of fwd, bwd and step latency)\n",
      "\n",
      "params per GPU:                                                         8.45 M  \n",
      "params of model = params per GPU * mp_size:                             0       \n",
      "fwd MACs per GPU:                                                       67.53 MMACs\n",
      "fwd flops per GPU:                                                      135.12 M\n",
      "fwd flops of model = fwd flops per GPU * mp_size:                       135.12 M\n",
      "fwd latency:                                                            2.35 ms \n",
      "fwd FLOPS per GPU = fwd flops per GPU / fwd latency:                    57.57 GFLOPS\n",
      "\n",
      "----------------------------- Aggregated Profile per GPU -----------------------------\n",
      "Top 1 modules in terms of params, MACs or fwd latency at different model depths:\n",
      "depth 0:\n",
      "    params      - {'GraphModule': '8.45 M'}\n",
      "    MACs        - {'GraphModule': '67.53 MMACs'}\n",
      "    fwd latency - {'GraphModule': '2.35 ms'}\n",
      "depth 1:\n",
      "    params      - {'Module': '8.45 M'}\n",
      "    MACs        - {'Module': '67.53 MMACs'}\n",
      "    fwd latency - {'Module': '2.13 ms'}\n",
      "\n",
      "------------------------------ Detailed Profile per GPU ------------------------------\n",
      "Each module profile is listed after its name in the following order: \n",
      "params, percentage of total params, MACs, percentage of total MACs, fwd latency, percentage of total fwd latency, fwd FLOPS\n",
      "\n",
      "Note: 1. A module can have torch.nn.module or torch.nn.functional to compute logits (e.g. CrossEntropyLoss). They are not counted as submodules, thus not to be printed out. However they make up the difference between a parent's MACs (or latency) and the sum of its submodules'.\n",
      "2. Number of floating-point operations is a theoretical estimation, thus FLOPS computed using that could be larger than the maximum system throughput.\n",
      "3. The fwd latency listed in the top module's profile is directly captured at the module forward function in PyTorch, thus it's less than the fwd latency shown above which is captured in DeepSpeed.\n",
      "\n",
      "GraphModule(\n",
      "  8.45 M = 100% Params, 67.53 MMACs = 100% MACs, 2.35 ms = 100% latency, 57.57 GFLOPS\n",
      "  (seq_blocks): Module(\n",
      "    8.45 M = 100% Params, 67.53 MMACs = 100% MACs, 2.13 ms = 90.7% latency, 63.47 GFLOPS\n",
      "    (0): BatchNorm1d(32 = 0% Params, 0 MACs = 0% MACs, 195.26 us = 8.32% latency, 1.31 MFLOPS, 16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (1): ReLU(0 = 0% Params, 0 MACs = 0% MACs, 45.78 us = 1.95% latency, 2.8 MFLOPS, inplace=True)\n",
      "    (2): Linear(34.82 K = 0.41% Params, 262.14 KMACs = 0.39% MACs, 133.04 us = 5.67% latency, 3.94 GFLOPS, in_features=16, out_features=2048, bias=True)\n",
      "    (3): ReLU(0 = 0% Params, 0 MACs = 0% MACs, 39.82 us = 1.7% latency, 411.49 MFLOPS, inplace=True)\n",
      "    (4): Linear(8.39 M = 99.35% Params, 67.11 MMACs = 99.37% MACs, 1.47 ms = 62.66% latency, 91.25 GFLOPS, in_features=2048, out_features=4096, bias=True)\n",
      "    (5): ReLU(0 = 0% Params, 0 MACs = 0% MACs, 70.33 us = 3% latency, 465.89 MFLOPS, inplace=True)\n",
      "    (6): Linear(20.48 K = 0.24% Params, 163.84 KMACs = 0.24% MACs, 137.33 us = 5.85% latency, 2.39 GFLOPS, in_features=4096, out_features=5, bias=True)\n",
      "    (7): ReLU(0 = 0% Params, 0 MACs = 0% MACs, 36.48 us = 1.55% latency, 1.1 MFLOPS, inplace=True)\n",
      "  )\n",
      ")\n",
      "\n",
      "\n",
      "\n",
      "def forward(self, x):\n",
      "    seq_blocks_0 = getattr(self.seq_blocks, \"0\")(x);  x = None\n",
      "    seq_blocks_1 = getattr(self.seq_blocks, \"1\")(seq_blocks_0);  seq_blocks_0 = None\n",
      "    seq_blocks_2 = getattr(self.seq_blocks, \"2\")(seq_blocks_1);  seq_blocks_1 = None\n",
      "    seq_blocks_3 = getattr(self.seq_blocks, \"3\")(seq_blocks_2);  seq_blocks_2 = None\n",
      "    seq_blocks_4 = getattr(self.seq_blocks, \"4\")(seq_blocks_3);  seq_blocks_3 = None\n",
      "    seq_blocks_5 = getattr(self.seq_blocks, \"5\")(seq_blocks_4);  seq_blocks_4 = None\n",
      "    seq_blocks_6 = getattr(self.seq_blocks, \"6\")(seq_blocks_5);  seq_blocks_5 = None\n",
      "    seq_blocks_7 = getattr(self.seq_blocks, \"7\")(seq_blocks_6);  seq_blocks_6 = None\n",
      "    return seq_blocks_7\n",
      "    \n",
      "# To see more debug info, please use `graph_module.print_readable()`\n",
      "------------------------------------------------------------------------------\n",
      "[2024-02-11 14:56:51,694] [INFO] [profiler.py:226:end_profile] Flops profiler finished\n"
     ]
    }
   ],
   "source": [
    "metric = MulticlassAccuracy(num_classes=5)\n",
    "# get_model_profile\n",
    "num_batchs = 5\n",
    "# This first loop is basically our search strategy,\n",
    "# in this case, it is a simple brute force search\n",
    "recorded_accs = []\n",
    "recorded_loss = []\n",
    "recorded_latencies = []\n",
    "recorded_model_sizes = []\n",
    "recorded_flops = []\n",
    "for i, config in enumerate(search_spaces):\n",
    "    print(i)\n",
    "    mg=reload_mg()\n",
    "    mg, _ = redefine_linear_Relu_transform_pass(\n",
    "    graph=mg, pass_args={\"config\": config})\n",
    "\n",
    "    j = 0\n",
    "    flops, macs, params = get_model_profile(model=mg.model, input_shape=tuple(dummy_in['x'].shape))\n",
    "    recorded_model_sizes.append(params)\n",
    "    recorded_flops.append(flops)\n",
    "\n",
    "    # this is the inner loop, where we also call it as a runner.\n",
    "    acc_avg, loss_avg = 0, 0\n",
    "    accs, losses = [], []\n",
    "\n",
    "    # Additional code for measuring latency\n",
    "    start_time = torch.cuda.Event(enable_timing=True)\n",
    "    end_time = torch.cuda.Event(enable_timing=True)\n",
    "\n",
    "    for inputs in data_module.train_dataloader():\n",
    "        xs, ys = inputs\n",
    "        preds = mg.model(xs)\n",
    "        loss = torch.nn.functional.cross_entropy(preds, ys)\n",
    "        acc = np.array(metric(preds, ys))\n",
    "        accs.append(acc)\n",
    "        losses.append(loss)\n",
    "        if j == 0:\n",
    "            start_time.record()\n",
    "        elif j == num_batchs:\n",
    "            end_time.record()\n",
    "            torch.cuda.synchronize()  # Wait for all GPU operations to finish\n",
    "            latency = start_time.elapsed_time(end_time) / num_batchs\n",
    "            recorded_latencies.append(latency)\n",
    "        if j > num_batchs:\n",
    "            break\n",
    "        j += 1\n",
    "    acc_avg = sum(accs) / len(accs)\n",
    "    loss_avg = sum(losses) / len(losses)\n",
    "    recorded_accs.append(acc_avg)\n",
    "    recorded_loss.append(loss_avg)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[1], [2], [4], [8], [16], [64]]\n",
      "[0.183333335178239, 0.2611111189637865, 0.10773809892790658, 0.05000000021287373, 0.1886904782482556, 0.14880952664784022]\n",
      "[tensor(1.6014, grad_fn=<DivBackward0>), tensor(1.6086, grad_fn=<DivBackward0>), tensor(1.6197, grad_fn=<DivBackward0>), tensor(1.6231, grad_fn=<DivBackward0>), tensor(1.6157, grad_fn=<DivBackward0>), tensor(1.6080, grad_fn=<DivBackward0>)]\n",
      "[0.6363391876220703, 0.6094016075134278, 0.6248576164245605, 0.705734395980835, 0.7767936229705811, 2.3720319747924803]\n",
      "['3.01 K', '10.09 K', '36.52 K', '138.53 K', '539.17 K', '8.45 M']\n",
      "['47.27 K', '159.66 K', '581.03 K', '2.21 M', '8.61 M', '135.12 M']\n"
     ]
    }
   ],
   "source": [
    "import csv\n",
    "with open('dataLab4.csv', mode='w', newline='') as file:\n",
    "    writer = csv.writer(file)\n",
    "    writer.writerow(['Search History', 'Recorded Accuracies', 'Recorded Losses', 'Recorded Latencies', 'Recorded Model Sizes', 'Recorded FLOPS'])\n",
    "    for i in range(len(search_history)):\n",
    "        writer.writerow([search_history[i], recorded_accs[i], recorded_loss[i], recorded_latencies[i], recorded_model_sizes[i], recorded_flops[i]])\n",
    "\n",
    "print(search_history)\n",
    "print(recorded_accs)\n",
    "print(recorded_loss)\n",
    "print(recorded_latencies)\n",
    "print(recorded_model_sizes)\n",
    "print(recorded_flops)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ./ch search --config configs/examples/jsc_tiny_my_lab4.toml\n",
    "# ./ch search --config configs/examples/jsc_tiny_my_lab4.toml\n",
    "# ./ch search_my --config configs/examples/jsc_tiny_my_lab4.toml\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "mase_new",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.1.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
